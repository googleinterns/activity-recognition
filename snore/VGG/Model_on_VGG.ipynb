{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model based on VGGish features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__VGGish__: 128-dimensional audio features extracted at 1Hz. The audio features were extracted using a VGG-inspired acoustic model described in Hershey et. al., trained on a preliminary version of YouTube-8M. The features were PCA-ed and quantized to be compatible with the audio features provided with YouTube-8M. They are stored as TensorFlow Record files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To my understanding: VGGish model turns audio classification problems into image classification problems. They create 2D image-like patches by computing log-mel spectrograms of multiple frames, and feed that into models. The input thus become some transformations of visual representation of spectrum of frequencies of the signal as it changes with time.\n",
    "\n",
    "Reference: https://arxiv.org/pdf/1609.09430.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PCA__: Two primary reasons for use\n",
    "- Data reduction: condense the information contained in a large number of original variables into a smaller set of new composite dimensions, with a minimum loss of information.\n",
    "- Interpretation: discover important features of a large data set that often reveals relationships that were previously unsuspected, thereby allowing interpretations that would not ordinarily result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from shutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory should be arranged in form:\n",
    "# .\n",
    "# ├── audioset_v1_embeddings\n",
    "# ├── class_labels_indices.csv\n",
    "# └── Model_on_VGG.ipynb\n",
    "\n",
    "path = \"audioset_v1_embeddings/\"\n",
    "eva = \"eval/\"\n",
    "bal = \"bal_train/\"\n",
    "unbal = \"unbal_train/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "\n",
    "## 1: convert .tfrecord info into .csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_label = pd.read_csv(\"class_labels_indices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_index_to_label(index_label, index):\n",
    "    # Maps index to readable labels\n",
    "    # Return label is a list, that could contain more than 1 item, but refer to the same label\n",
    "    # Return labels are all in lower form, no CAP\n",
    "    # e.g. ['male speech', 'man speaking']\n",
    "    #\n",
    "    # input: pandas.DataFrame index_to_label_df, int index\n",
    "    # output: list readable_label\n",
    "    \n",
    "    # Get the real index of df, in case of mismatch\n",
    "    index = index_label.loc[index_label['index'] == index].index[0] \n",
    "    labels = [label.strip().lower() for label in index_label.iloc[index]['display_name'].split(\",\")]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_index(index_label, label):\n",
    "    # Maps readable labels to index\n",
    "    #\n",
    "    # input: pandas.DataFrame index_to_label_df, str label\n",
    "    # output: int index\n",
    "    label = label.lower()\n",
    "    labelCap = label.capitalize()\n",
    "    for index, row in index_label.iterrows():\n",
    "        labels = [label.strip() for label in row['display_name'].split(\",\")]\n",
    "        if label in labels or labelCap in labels:\n",
    "            return index_label.iloc[index]['index']\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(filename):\n",
    "    # Read in a tfrecord file\n",
    "    # Store information in list of lists\n",
    "    #\n",
    "    # input: str filename\n",
    "    # output: pandas dataframe with columns:\n",
    "    #        [str video_id, float start_time, float end_time, list label_index, list embed]\n",
    "    \n",
    "    if not filename.endswith('.tfrecord'):\n",
    "        print(\"This file is not a .tfrecord file.\")\n",
    "        return\n",
    "    raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "    return_df = pd.DataFrame(columns=\n",
    "                             ['video_id', 'start_time_seconds', 'end_time_seconds', 'labels', 'audio_embedding'])\n",
    "    for raw_record in raw_dataset:\n",
    "        cur_record_list = []\n",
    "        example = tf.train.SequenceExample()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        \n",
    "        cur_record_list.append(example.context.feature['video_id'].bytes_list.value[0].decode(\"utf-8\"))\n",
    "        cur_record_list.append(example.context.feature['start_time_seconds'].float_list.value[0])\n",
    "        cur_record_list.append(example.context.feature['end_time_seconds'].float_list.value[0])\n",
    "        cur_record_list.append(example.context.feature['labels'].int64_list.value)\n",
    "        \n",
    "        # Original embeddings are stored in hex format, now convert them to readable int\n",
    "        hexembed = example.feature_lists.feature_list['audio_embedding'].feature[0].bytes_list.value[0].hex()\n",
    "        arrayembed = [int(hexembed[i:i+2], 16) for i in range(0, len(hexembed), 2)]\n",
    "        cur_record_list.append(arrayembed)\n",
    "        \n",
    "        return_df.loc[len(return_df)] = cur_record_list\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_tfrecord_dir(dir_path, dest):\n",
    "    # Read and concat all tfrecord files in a directory\n",
    "    # and save to csv in appending mode\n",
    "    #\n",
    "    # input: str directory_path, (must contain '/' in the end, e.g. 'unbal_train/')\n",
    "    # output: int count_tfrecord_files\n",
    "    cnt = 0\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".tfrecord\"):\n",
    "            df = read_tfrecord(dir_path+file)\n",
    "            # Original method, that read all data into a df, then store it as csv\n",
    "            # takes too much RAM, and causes crashes when reach limit\n",
    "            # Now update to batch save to avoid exploding uses of RAM\n",
    "            # But still, CPU usage is too high, hope to upgrade later\n",
    "            if cnt == 0:\n",
    "                df.to_csv(dir_path+dest, index=False, header=True)\n",
    "            else:\n",
    "                df.to_csv(dir_path+dest, mode='a', index=False, header=False)\n",
    "            cnt += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_csv_tfrecord_dir(path+eva, 'eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_csv_tfrecord_dir(path+bal, 'bal_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a looooooooooong time!\n",
    "# convert_csv_tfrecord_dir(path+unbal, 'unbal_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "\n",
    "## 2: prepare data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_info = pd.read_csv(path+eva+\"eval.csv\")\n",
    "bal_info = pd.read_csv(path+bal+\"bal_train.csv\")\n",
    "unbal_info = pd.read_csv(path+unbal+\"unbal_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>start_time_seconds</th>\n",
       "      <th>end_time_seconds</th>\n",
       "      <th>labels</th>\n",
       "      <th>audio_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2Hj6ogt1TJo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>[86, 90, 91]</td>\n",
       "      <td>[42, 94, 128, 51, 200, 113, 115, 50, 169, 120,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2HZcxlRs-hg</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[300, 316, 319]</td>\n",
       "      <td>[0, 255, 0, 0, 179, 248, 255, 106, 50, 73, 95,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wRgH7HvmSiE</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[46, 485]</td>\n",
       "      <td>[100, 65, 207, 138, 118, 81, 146, 35, 219, 166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wRKCb1rOGT8</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>[420]</td>\n",
       "      <td>[49, 255, 0, 0, 122, 236, 115, 51, 223, 203, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wRvtuIFoqCM</td>\n",
       "      <td>170.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>[423, 476]</td>\n",
       "      <td>[35, 83, 161, 88, 201, 166, 42, 110, 132, 212,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  start_time_seconds  end_time_seconds           labels  \\\n",
       "0  2Hj6ogt1TJo                80.0              90.0     [86, 90, 91]   \n",
       "1  2HZcxlRs-hg                30.0              40.0  [300, 316, 319]   \n",
       "2  wRgH7HvmSiE                30.0              40.0        [46, 485]   \n",
       "3  wRKCb1rOGT8                40.0              50.0            [420]   \n",
       "4  wRvtuIFoqCM               170.0             180.0       [423, 476]   \n",
       "\n",
       "                                     audio_embedding  \n",
       "0  [42, 94, 128, 51, 200, 113, 115, 50, 169, 120,...  \n",
       "1  [0, 255, 0, 0, 179, 248, 255, 106, 50, 73, 95,...  \n",
       "2  [100, 65, 207, 138, 118, 81, 146, 35, 219, 166...  \n",
       "3  [49, 255, 0, 0, 122, 236, 115, 51, 223, 203, 6...  \n",
       "4  [35, 83, 161, 88, 201, 166, 42, 110, 132, 212,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_label(df, label):\n",
    "    # Create a sub-df from df where labeled as <label>\n",
    "    #\n",
    "    # input: df df, label string\n",
    "    # output: df df_elements_with_<label>\n",
    "    index = map_label_to_index(index_label, label)\n",
    "    # labels read from csv file become str instead of list\n",
    "    mask = df.labels.apply(lambda x: True if index in [int(l) for l in x[1:-1].split(',')] else False)\n",
    "    df1 = df[mask]\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sub_df(df, df_subset):\n",
    "    # (df - df_subset), where df_subset is a subset of df\n",
    "    # \n",
    "    # input: df df, df subset_of_df\n",
    "    # output: df \n",
    "    df_new = df.merge(df_subset, how='left', indicator=True)\n",
    "    df_new = df_new[df_new['_merge'] == 'left_only']\n",
    "    del df_new['_merge']\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sample(df, size):\n",
    "    # Get a subset from df, where contains n elemets, randomly sampled from df\n",
    "    #\n",
    "    # input: df df, size number_of_elements_in_subset\n",
    "    # output: df\n",
    "    return df.sample(n = size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_add_col_with_value(df, col_name, value):\n",
    "    # Add a column to given df with specified value\n",
    "    #\n",
    "    # input: df df, col_name column_name, value default_value_of_new_column\n",
    "    # output: df df\n",
    "    df[col_name] = value\n",
    "    return df\n",
    "\n",
    "def df_get_col_value_as_list(df, col_name):\n",
    "    # Return the value of specific column as list\n",
    "    # Do something different to 'audio_embedding', turn list of strs to list of ints\n",
    "    #\n",
    "    # input: df df, col_name column_name\n",
    "    # output: list column_values\n",
    "    if col_name != 'audio_embedding':\n",
    "        return df[col_name].tolist()\n",
    "    else:\n",
    "        tmp = df[col_name].tolist()\n",
    "        return [[int(num) for num in embed[1:-1].split(',')] for embed in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_get_labelled_unlabelled(df, label, ratio_of_label_unlabel):\n",
    "    # Create a new df, that contains all rows with <label>\n",
    "    # and n * len(<label>) rows without <label>\n",
    "    # The new df contains a new columns that indicate whether contain <label>\n",
    "    #\n",
    "    # input: df df, str label, int ratio_of_label:unlabel\n",
    "    # output: df new_df\n",
    "    labelled = get_df_with_label(df, label)\n",
    "    unlabelled = drop_sub_df(df, labelled)\n",
    "    unlabelled = df_sample(unlabelled, len(labelled) * ratio_of_label_unlabel)\n",
    "    labelled = df_add_col_with_value(labelled, 'y', 1)\n",
    "    unlabelled = df_add_col_with_value(unlabelled, 'y', 0)\n",
    "    new_df = pd.concat([labelled, unlabelled],ignore_index=True)\n",
    "    # Shuffle df\n",
    "    new_df = new_df.iloc[np.random.permutation(len(new_df))]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df_size(dfs, a, b, c):\n",
    "    # Rebalance the size of three dataframes train, val, test\n",
    "    # Reset the ratio of train:val:test to a:b:c\n",
    "    # Keep the original ratio of label:unlabel in new dfs\n",
    "    #\n",
    "    # input: list[df] dfs, int a, int b, int c\n",
    "    # output: list[df] rebalanced_dfs\n",
    "    total = pd.concat(dfs, ignore_index=True)\n",
    "    labeled = total.loc[total['y'] == 1]\n",
    "    unlabeled = drop_sub_df(total, labeled)\n",
    "    \n",
    "    train_labeled = labeled.sample(frac=(a/(a+b+c)))\n",
    "    tmp_labeled = drop_sub_df(labeled, train_labeled)\n",
    "    val_labeled = tmp_labeled.sample(frac=(b/(b+c)))\n",
    "    test_labeled = drop_sub_df(tmp_labeled, val_labeled)\n",
    "    \n",
    "    train_unlabeled = unlabeled.sample(frac=(a/(a+b+c)))\n",
    "    tmp_unlabeled = drop_sub_df(unlabeled, train_unlabeled)\n",
    "    val_unlabeled = tmp_unlabeled.sample(frac=(b/(b+c)))\n",
    "    test_unlabeled = drop_sub_df(tmp_unlabeled, val_unlabeled)\n",
    "    \n",
    "    train = pd.concat([train_labeled, train_unlabeled], ignore_index=True)\n",
    "    val = pd.concat([val_labeled, val_unlabeled], ignore_index=True)\n",
    "    test = pd.concat([test_labeled, test_unlabeled], ignore_index=True)\n",
    "    return [train, val, test]\n",
    "\n",
    "def data_for_model(dfs, label, ratio_of_label_unlabel, a=0, b=0, c=0, norm=False):\n",
    "    # Combine previous functions and form x, y value lists for model\n",
    "    # Return train_x/y, val_x/y, test_x/y, in one call\n",
    "    # ratio_of_label_unlabel is used for df_get_labelled_unlablled, as the ratio of label:unlabel\n",
    "    # a, b, c is ratio of train:val:test, if they are not specified, then do not modify\n",
    "    # norm decide whether normalize the embeddings.\n",
    "    #\n",
    "    # input: list[df] dfs, str label, int ratio_of_label:unlabel, int a, int b, int c, bool norm\n",
    "    # output: lists train/val/test_x/y\n",
    "    train = df_get_labelled_unlabelled(dfs[0], label, ratio_of_label_unlabel)\n",
    "    val = df_get_labelled_unlabelled(dfs[1], label, ratio_of_label_unlabel)\n",
    "    test = df_get_labelled_unlabelled(dfs[2], label, ratio_of_label_unlabel)\n",
    "    \n",
    "    if a!=0 or b!=0 or c!=0:\n",
    "        train, val, test = balance_df_size([train, val, test], a, b, c)\n",
    "        \n",
    "    train_x = df_get_col_value_as_list(train, 'audio_embedding')\n",
    "    train_y = df_get_col_value_as_list(train, 'y')\n",
    "    val_x = df_get_col_value_as_list(val, 'audio_embedding')\n",
    "    val_y = df_get_col_value_as_list(val, 'y')\n",
    "    test_x = df_get_col_value_as_list(test, 'audio_embedding')\n",
    "    test_y = df_get_col_value_as_list(test, 'y')\n",
    "    if norm:\n",
    "        train_x = [[n/255 for n in lst] for lst in train_x]\n",
    "        val_x = [[n/255 for n in lst] for lst in val_x]\n",
    "        test_x = [[n/255 for n in lst] for lst in test_x]\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(history, filename):\n",
    "    # Visulize training result\n",
    "    #\n",
    "    # input: history output_of_model.fit\n",
    "    # output: None\n",
    "    history_dict = history.history\n",
    "    history_dict.keys()\n",
    "    acc = history_dict['accuracy']\n",
    "    val_acc = history_dict['val_accuracy']\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7))\n",
    "    \n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    ax1.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "    ax1.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    ax1.set_title('Training and validation loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    ax2.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    ax2.set_title('Training and validation accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend(loc='lower right')\n",
    "    \n",
    "#     plt.show()\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "def print_test_result(model, test_x, text_y):\n",
    "    results = model.evaluate(test_x, test_y, verbose=2)\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pic_with_keyword(src_dirs, dst_dir, keywords):\n",
    "    # Copy the targeted files from src_dirs to dst_dir\n",
    "    # \n",
    "    # input: list[str] src_dirs, str dst_dir, list[str] keywords\n",
    "    # output: None\n",
    "    rmtree(dst_dir)\n",
    "    os.mkdir(dst_dir)\n",
    "    for src_dir in src_dirs:\n",
    "        files = [f for f in listdir(src_dir) if isfile(join(src_dir, f))]\n",
    "        mov_files = [f for f in files if all(k in f for k in keywords)]\n",
    "        for f in mov_files:\n",
    "            copyfile(src_dir+f, dst_dir+f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "\n",
    "## 3.1: build and train model [exploration]\n",
    "### First trial: \n",
    "Focus on balanced data input, where ratio is set to 1.  \n",
    "Check results1/README.md for detail information:  \n",
    "https://github.com/googleinterns/activity-recognition/blob/snore-AudioPrep/snore/VGG/results1/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config_train_1(data, activation, optimizer, metrics, epochs):\n",
    "    # Get train, val, test sets, and config to train model\n",
    "    #\n",
    "    # input: list[list] data, str activation, str optimizer, list[str] metrics, int epochs\n",
    "    # output: history output_of_model.fit\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = data\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(128,)),\n",
    "        keras.layers.Dense(128, activation=activation),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=metrics)\n",
    "    history = model.fit(train_x, train_y,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=(val_x, val_y),\n",
    "                    verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [1, 20, 40]\n",
    "activation_list = ['elu', 'exponential', 'relu', 'selu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh']\n",
    "optimizer_list = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']\n",
    "metrics_list = [['accuracy']]\n",
    "epochs_list = [20, 30, 40, 50]\n",
    "\n",
    "for ratio in ratio_list:\n",
    "    data = data_for_model([unbal_info, bal_info, eval_info], 'snoring', ratio)\n",
    "    for activation in activation_list:\n",
    "        for optimizer in optimizer_list:\n",
    "            for metrics in metrics_list:\n",
    "                for epochs in epochs_list:\n",
    "                    history = model_config_train_1(data, activation, optimizer, metrics, epochs)\n",
    "                    visualize_training(history, \n",
    "                                       'results1/'+str(ratio)+'_'+activation+'_'+\n",
    "                                       optimizer+'_'+'_'.join(metrics)+'_'+str(epochs)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "\n",
    "## 3.2: build and train model [increase val size]\n",
    "### Second trial: \n",
    "__Set train : val : test to 8 : 1 : 1__  \n",
    "__Give unbalanced data for training, in order to get larger training size.__  \n",
    "Check results2/README.md for detail information:  \n",
    "https://github.com/googleinterns/activity-recognition/blob/snore-AudioPrep/snore/VGG/results2/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config_train_2(data, activation, optimizer, metrics, epochs):\n",
    "    # Get train, val, test sets, and config to train model\n",
    "    #\n",
    "    # input: list[list] data, str activation, str optimizer, list[str] metrics, int epochs\n",
    "    # output: history output_of_model.fit\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = data\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(128,)),\n",
    "        keras.layers.Dense(128, activation=activation),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=metrics)\n",
    "    history = model.fit(train_x, train_y,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=(val_x, val_y),\n",
    "                    verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [1, 10]\n",
    "activation_list = ['elu', 'relu', 'selu', 'sigmoid', 'softsign', 'tanh']\n",
    "optimizer_list = ['adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop']\n",
    "metrics_list = [['accuracy']]\n",
    "epochs_list = [20, 40, 60, 80, 100]\n",
    "\n",
    "for ratio in ratio_list:\n",
    "    data = data_for_model([unbal_info, bal_info, eval_info], 'snoring', ratio, a=8, b=1, c=1)\n",
    "    for activation in activation_list:\n",
    "        for optimizer in optimizer_list:\n",
    "            for metrics in metrics_list:\n",
    "                for epochs in epochs_list:\n",
    "                    history = model_config_train_2(data, activation, optimizer, metrics, epochs)\n",
    "                    visualize_training(history, \n",
    "                                       'results2/'+str(ratio)+'_'+activation+'_'+\n",
    "                                       optimizer+'_'+'_'.join(metrics)+'_'+str(epochs)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "-------------------------------\n",
    "\n",
    "## 3.3: build and train model [normalization]\n",
    "### Third trial: \n",
    "__Since VGG turns audio problem to image problem, the 128-dimension embeddings are at range [0, 255]. Should be better if we normalize them.__  \n",
    "__Train : val : test = 8 : 1 : 1__  \n",
    "Check results3/README.md for detail information:  \n",
    "https://github.com/googleinterns/activity-recognition/blob/snore-AudioPrep/snore/VGG/results3/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config_train_3(data, activation, optimizer, metrics, epochs):\n",
    "    # Get train, val, test sets, and config to train model\n",
    "    #\n",
    "    # input: list[list] data, str activation, str optimizer, list[str] metrics, int epochs\n",
    "    # output: history output_of_model.fit\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = data\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(128,)),\n",
    "        keras.layers.Dense(128, activation=activation),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=metrics)\n",
    "    history = model.fit(train_x, train_y,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=(val_x, val_y),\n",
    "                    verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [1]\n",
    "activation_list = ['elu', 'exponential', 'relu', 'selu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh']\n",
    "optimizer_list = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']\n",
    "metrics_list = [['accuracy']]\n",
    "epochs_list = [30, 50]\n",
    "\n",
    "for ratio in ratio_list:\n",
    "    data = data_for_model([unbal_info, bal_info, eval_info], \n",
    "                          'snoring', 1, a=8, b=1, c=1, norm=True)\n",
    "    for activation in activation_list:\n",
    "        for optimizer in optimizer_list:\n",
    "            for metrics in metrics_list:\n",
    "                for epochs in epochs_list:\n",
    "                    history = model_config_train_3(data, activation, optimizer, metrics, epochs)\n",
    "                    visualize_training(history, \n",
    "                                       'results3/trail3_'+str(ratio)+'_'+activation+'_'+\n",
    "                                       optimizer+'_'+'_'.join(metrics)+'_'+str(epochs)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_pic_with_keyword(['results1/', 'results3/'], 'tmp/', ['1_elu_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
