{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite .npz parameters into vars in .py\n",
    "\n",
    "so we can use it in Android Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(\"vggish_pca_params.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pca_means', 'pca_eigen_vectors']\n"
     ]
    }
   ],
   "source": [
    "print(tmp.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp['pca_means'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp['pca_eigen_vectors'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "f = open(\"vggish_pca_params.py\", \"w\")\n",
    "\n",
    "f.write(\"pca_means = [\")\n",
    "cnt = 0\n",
    "for i in tmp['pca_means']:\n",
    "    if cnt == 0:\n",
    "        f.write(str(i))\n",
    "    else:\n",
    "        f.write(',' + str(i))\n",
    "    cnt += 1\n",
    "f.write(']\\n\\n')\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write(\"pca_eigen_vectors = [\")\n",
    "cnt1 = 0\n",
    "for i in tmp['pca_eigen_vectors']:\n",
    "    if cnt1 != 0:\n",
    "        f.write(',')\n",
    "    cnt2 = 0\n",
    "    for j in i:\n",
    "        if cnt2 == 0:\n",
    "            f.write('[' + str(j))\n",
    "        else:\n",
    "            f.write(',' + str(j))\n",
    "        cnt2 += 1\n",
    "    f.write(']')\n",
    "    print(cnt2)\n",
    "    cnt1 += 1\n",
    "f.write(']\\n\\n')\n",
    "print(cnt)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert VGGish slim model into .tflite model\n",
    "\n",
    "so we can use it in Android Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ohjerry/.local/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# vggish_slim.py\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tf_slim as slim\n",
    "\n",
    "import vggish_params as params\n",
    "\n",
    "\n",
    "def define_vggish_slim(training=False):\n",
    "  \"\"\"Defines the VGGish TensorFlow model.\n",
    "\n",
    "  All ops are created in the current default graph, under the scope 'vggish/'.\n",
    "\n",
    "  The input is a placeholder named 'vggish/input_features' of type float32 and\n",
    "  shape [batch_size, num_frames, num_bands] where batch_size is variable and\n",
    "  num_frames and num_bands are constants, and [num_frames, num_bands] represents\n",
    "  a log-mel-scale spectrogram patch covering num_bands frequency bands and\n",
    "  num_frames time frames (where each frame step is usually 10ms). This is\n",
    "  produced by computing the stabilized log(mel-spectrogram + params.LOG_OFFSET).\n",
    "  The output is an op named 'vggish/embedding' which produces the activations of\n",
    "  a 128-D embedding layer, which is usually the penultimate layer when used as\n",
    "  part of a full model with a final classifier layer.\n",
    "\n",
    "  Args:\n",
    "    training: If true, all parameters are marked trainable.\n",
    "\n",
    "  Returns:\n",
    "    The op 'vggish/embeddings'.\n",
    "  \"\"\"\n",
    "  # Defaults:\n",
    "  # - All weights are initialized to N(0, INIT_STDDEV).\n",
    "  # - All biases are initialized to 0.\n",
    "  # - All activations are ReLU.\n",
    "  # - All convolutions are 3x3 with stride 1 and SAME padding.\n",
    "  # - All max-pools are 2x2 with stride 2 and SAME padding.\n",
    "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      weights_initializer=tf.truncated_normal_initializer(\n",
    "                          stddev=params.INIT_STDDEV),\n",
    "                      biases_initializer=tf.zeros_initializer(),\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      trainable=training), \\\n",
    "       slim.arg_scope([slim.conv2d],\n",
    "                      kernel_size=[3, 3], stride=1, padding='SAME'), \\\n",
    "       slim.arg_scope([slim.max_pool2d],\n",
    "                      kernel_size=[2, 2], stride=2, padding='SAME'), \\\n",
    "       tf.variable_scope('vggish'):\n",
    "    # Input: a batch of 2-D log-mel-spectrogram patches.\n",
    "    features = tf.placeholder(\n",
    "        tf.float32, shape=(None, params.NUM_FRAMES, params.NUM_BANDS),\n",
    "        name='input_features')\n",
    "    # Reshape to 4-D so that we can convolve a batch with conv2d().\n",
    "    net = tf.reshape(features, [-1, params.NUM_FRAMES, params.NUM_BANDS, 1])\n",
    "\n",
    "    # The VGG stack of alternating convolutions and max-pools.\n",
    "    net = slim.conv2d(net, 64, scope='conv1')\n",
    "    net = slim.max_pool2d(net, scope='pool1')\n",
    "    net = slim.conv2d(net, 128, scope='conv2')\n",
    "    net = slim.max_pool2d(net, scope='pool2')\n",
    "    net = slim.repeat(net, 2, slim.conv2d, 256, scope='conv3')\n",
    "    net = slim.max_pool2d(net, scope='pool3')\n",
    "    net = slim.repeat(net, 2, slim.conv2d, 512, scope='conv4')\n",
    "    net = slim.max_pool2d(net, scope='pool4')\n",
    "\n",
    "    # Flatten before entering fully-connected layers\n",
    "    net = slim.flatten(net)\n",
    "    net = slim.repeat(net, 2, slim.fully_connected, 4096, scope='fc1')\n",
    "    # The embedding layer.\n",
    "    net = slim.fully_connected(net, params.EMBEDDING_SIZE, scope='fc2')\n",
    "    return tf.identity(net, name='embedding')\n",
    "\n",
    "\n",
    "def load_vggish_slim_checkpoint(session, checkpoint_path):\n",
    "  \"\"\"Loads a pre-trained VGGish-compatible checkpoint.\n",
    "\n",
    "  This function can be used as an initialization function (referred to as\n",
    "  init_fn in TensorFlow documentation) which is called in a Session after\n",
    "  initializating all variables. When used as an init_fn, this will load\n",
    "  a pre-trained checkpoint that is compatible with the VGGish model\n",
    "  definition. Only variables defined by VGGish will be loaded.\n",
    "\n",
    "  Args:\n",
    "    session: an active TensorFlow session.\n",
    "    checkpoint_path: path to a file containing a checkpoint that is\n",
    "      compatible with the VGGish model definition.\n",
    "  \"\"\"\n",
    "  # Get the list of names of all VGGish variables that exist in\n",
    "  # the checkpoint (i.e., all inference-mode VGGish variables).\n",
    "  with tf.Graph().as_default():\n",
    "    define_vggish_slim(training=False)\n",
    "    vggish_var_names = [v.name for v in tf.global_variables()]\n",
    "\n",
    "  # Get the list of all currently existing variables that match\n",
    "  # the list of variable names we just computed.\n",
    "  vggish_vars = [v for v in tf.global_variables() if v.name in vggish_var_names]\n",
    "\n",
    "  # Use a Saver to restore just the variables selected above.\n",
    "  saver = tf.train.Saver(vggish_vars, name='vggish_load_pretrained',\n",
    "                         write_version=1)\n",
    "  saver.restore(session, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerry vggish_input.py: start wavfile_to_examples()\n",
      "44100\n",
      "[[789 789]\n",
      " [507 507]\n",
      " [154 154]\n",
      " ...\n",
      " [  0   0]\n",
      " [  0   0]\n",
      " [  0   0]]\n",
      "(432488, 2)\n",
      "Jerry vggish_input.py: start waveform_to_examples()\n",
      "Jerry vggish_input.py: before resample\n",
      "after resample\n",
      "(156912,)\n",
      "Jerry vggish_input.py: before log_mel\n",
      "Jerry vggish_input.py: after log_mel\n",
      "Jerry vggish_input.py: finish waveform_to_examples()\n",
      "(10, 96, 64)\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "WARNING:tensorflow:From /home/ohjerry/.local/lib/python3.7/site-packages/tensorflow/lite/python/util.py:267: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/ohjerry/.local/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:359: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 18 variables.\n",
      "INFO:tensorflow:Converted 18 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "import soundfile\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "\n",
    "ckpt = 'vggish_model.ckpt'\n",
    "examples_batch = vggish_input.wavfile_to_examples('test.wav')\n",
    "pproc = vggish_postprocess.Postprocessor()\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # Define the model in inference mode, load the checkpoint, and\n",
    "    # locate input and output tensors.\n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, ckpt)\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    # Run inference and postprocessing.\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "    converter = tf.lite.TFLiteConverter.from_session(sess, [features_tensor], [embedding_tensor])\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"saved_model/vggish_feature_extraction_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[151,  26, 146, ..., 102, 197, 255],\n",
       "       [154,  26, 168, ...,   0, 138, 255],\n",
       "       [155,  25, 164, ...,  79, 131, 255],\n",
       "       ...,\n",
       "       [175,   8, 146, ..., 147,  69, 255],\n",
       "       [175,   8, 146, ..., 147,  69, 255],\n",
       "       [175,   8, 146, ..., 147,  69, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The original, \"should be\", output after post-process\n",
    "postprocessed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.51773083 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.6152872  0.         0.10851269 0.\n",
      "  0.         0.11004636 0.         0.         0.06447921 0.\n",
      "  0.         0.         0.02334112 0.36665872 0.15915178 0.\n",
      "  0.         0.         0.         0.38003775 0.11620829 0.31883034\n",
      "  0.         0.         0.         1.068456   0.35640162 0.\n",
      "  0.         0.         0.8330307  0.         0.         0.\n",
      "  0.         0.90416443 0.         0.19184536 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.3652949  0.\n",
      "  0.         1.1452391  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24345502\n",
      "  0.32170972 0.         0.         0.05670428 0.         0.\n",
      "  0.         0.6894784  0.         0.13961355 0.2678038  0.\n",
      "  0.         0.24759701 0.         1.3265727  0.         0.94142413\n",
      "  0.         0.         0.         0.04135835 0.         0.44179863\n",
      "  0.12995909 0.04707202 0.         0.         0.         0.\n",
      "  0.22660175 0.         0.         0.         0.20387134 0.05225535\n",
      "  0.         0.         0.33169717 0.5493568  0.         0.\n",
      "  0.         0.3283705  0.         0.         0.         0.\n",
      "  0.35539    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Test .tflite model\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"saved_model/vggish_feature_extraction_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array([examples_batch[0]], dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[151,  26, 146,  46, 253,  83, 140,  92, 141, 210, 144,  80, 140,\n",
       "        234, 173, 102,  48, 181, 129, 147,  29, 211,  90, 111, 124, 164,\n",
       "        220, 187,  41,  23, 126, 152, 141, 103,  52, 141, 160,  57,  76,\n",
       "        184, 115,  97,   0, 200,   0, 101,  65, 152, 112, 255, 255,  43,\n",
       "         93, 147, 144, 149, 113,   0, 110, 195, 222,  48,  22, 255, 166,\n",
       "        146, 119, 156, 113,   0, 241, 101, 255, 128, 138, 128,  99, 203,\n",
       "        158, 232,  11, 246, 184, 140, 175, 128, 255, 198,  94,  92,   0,\n",
       "        255, 203, 187,  79, 200,   0,   0, 255, 147, 255,   0, 225,  59,\n",
       "          0, 255, 182, 135,   0, 236, 230, 239,   0,  83, 145,   0, 126,\n",
       "         81,   0, 150,  62,   8,   0, 146, 197, 102, 197, 255]],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pproc.postprocess(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
