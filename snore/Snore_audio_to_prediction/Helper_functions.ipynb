{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite .npz parameters into vars in .py\n",
    "\n",
    "so we can use it in Android Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(\"vggish_pca_params.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pca_means', 'pca_eigen_vectors']\n"
     ]
    }
   ],
   "source": [
    "print(tmp.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp['pca_means'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp['pca_eigen_vectors'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "f = open(\"vggish_pca_params.py\", \"w\")\n",
    "\n",
    "f.write(\"pca_means = [\")\n",
    "cnt = 0\n",
    "for i in tmp['pca_means']:\n",
    "    if cnt == 0:\n",
    "        f.write(str(i))\n",
    "    else:\n",
    "        f.write(',' + str(i))\n",
    "    cnt += 1\n",
    "f.write(']\\n\\n')\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write(\"pca_eigen_vectors = [\")\n",
    "cnt1 = 0\n",
    "for i in tmp['pca_eigen_vectors']:\n",
    "    if cnt1 != 0:\n",
    "        f.write(',')\n",
    "    cnt2 = 0\n",
    "    for j in i:\n",
    "        if cnt2 == 0:\n",
    "            f.write('[' + str(j))\n",
    "        else:\n",
    "            f.write(',' + str(j))\n",
    "        cnt2 += 1\n",
    "    f.write(']')\n",
    "    print(cnt2)\n",
    "    cnt1 += 1\n",
    "f.write(']\\n\\n')\n",
    "print(cnt)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert VGGish slim model into .tflite model\n",
    "\n",
    "so we can use it in Android Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggish_slim.py\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tf_slim as slim\n",
    "\n",
    "import vggish_params as params\n",
    "\n",
    "\n",
    "def define_vggish_slim(training=False):\n",
    "  \"\"\"Defines the VGGish TensorFlow model.\n",
    "\n",
    "  All ops are created in the current default graph, under the scope 'vggish/'.\n",
    "\n",
    "  The input is a placeholder named 'vggish/input_features' of type float32 and\n",
    "  shape [batch_size, num_frames, num_bands] where batch_size is variable and\n",
    "  num_frames and num_bands are constants, and [num_frames, num_bands] represents\n",
    "  a log-mel-scale spectrogram patch covering num_bands frequency bands and\n",
    "  num_frames time frames (where each frame step is usually 10ms). This is\n",
    "  produced by computing the stabilized log(mel-spectrogram + params.LOG_OFFSET).\n",
    "  The output is an op named 'vggish/embedding' which produces the activations of\n",
    "  a 128-D embedding layer, which is usually the penultimate layer when used as\n",
    "  part of a full model with a final classifier layer.\n",
    "\n",
    "  Args:\n",
    "    training: If true, all parameters are marked trainable.\n",
    "\n",
    "  Returns:\n",
    "    The op 'vggish/embeddings'.\n",
    "  \"\"\"\n",
    "  # Defaults:\n",
    "  # - All weights are initialized to N(0, INIT_STDDEV).\n",
    "  # - All biases are initialized to 0.\n",
    "  # - All activations are ReLU.\n",
    "  # - All convolutions are 3x3 with stride 1 and SAME padding.\n",
    "  # - All max-pools are 2x2 with stride 2 and SAME padding.\n",
    "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      weights_initializer=tf.truncated_normal_initializer(\n",
    "                          stddev=params.INIT_STDDEV),\n",
    "                      biases_initializer=tf.zeros_initializer(),\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      trainable=training), \\\n",
    "       slim.arg_scope([slim.conv2d],\n",
    "                      kernel_size=[3, 3], stride=1, padding='SAME'), \\\n",
    "       slim.arg_scope([slim.max_pool2d],\n",
    "                      kernel_size=[2, 2], stride=2, padding='SAME'), \\\n",
    "       tf.variable_scope('vggish'):\n",
    "    # Input: a batch of 2-D log-mel-spectrogram patches.\n",
    "    features = tf.placeholder(\n",
    "        tf.float32, shape=(None, params.NUM_FRAMES, params.NUM_BANDS),\n",
    "        name='input_features')\n",
    "    # Reshape to 4-D so that we can convolve a batch with conv2d().\n",
    "    net = tf.reshape(features, [-1, params.NUM_FRAMES, params.NUM_BANDS, 1])\n",
    "\n",
    "    # The VGG stack of alternating convolutions and max-pools.\n",
    "    net = slim.conv2d(net, 64, scope='conv1')\n",
    "    net = slim.max_pool2d(net, scope='pool1')\n",
    "    net = slim.conv2d(net, 128, scope='conv2')\n",
    "    net = slim.max_pool2d(net, scope='pool2')\n",
    "    net = slim.repeat(net, 2, slim.conv2d, 256, scope='conv3')\n",
    "    net = slim.max_pool2d(net, scope='pool3')\n",
    "    net = slim.repeat(net, 2, slim.conv2d, 512, scope='conv4')\n",
    "    net = slim.max_pool2d(net, scope='pool4')\n",
    "\n",
    "    # Flatten before entering fully-connected layers\n",
    "    net = slim.flatten(net)\n",
    "    net = slim.repeat(net, 2, slim.fully_connected, 4096, scope='fc1')\n",
    "    # The embedding layer.\n",
    "    net = slim.fully_connected(net, params.EMBEDDING_SIZE, scope='fc2')\n",
    "    return tf.identity(net, name='embedding')\n",
    "\n",
    "\n",
    "def load_vggish_slim_checkpoint(session, checkpoint_path):\n",
    "  \"\"\"Loads a pre-trained VGGish-compatible checkpoint.\n",
    "\n",
    "  This function can be used as an initialization function (referred to as\n",
    "  init_fn in TensorFlow documentation) which is called in a Session after\n",
    "  initializating all variables. When used as an init_fn, this will load\n",
    "  a pre-trained checkpoint that is compatible with the VGGish model\n",
    "  definition. Only variables defined by VGGish will be loaded.\n",
    "\n",
    "  Args:\n",
    "    session: an active TensorFlow session.\n",
    "    checkpoint_path: path to a file containing a checkpoint that is\n",
    "      compatible with the VGGish model definition.\n",
    "  \"\"\"\n",
    "  # Get the list of names of all VGGish variables that exist in\n",
    "  # the checkpoint (i.e., all inference-mode VGGish variables).\n",
    "  with tf.Graph().as_default():\n",
    "    define_vggish_slim(training=False)\n",
    "    vggish_var_names = [v.name for v in tf.global_variables()]\n",
    "\n",
    "  # Get the list of all currently existing variables that match\n",
    "  # the list of variable names we just computed.\n",
    "  vggish_vars = [v for v in tf.global_variables() if v.name in vggish_var_names]\n",
    "\n",
    "  # Use a Saver to restore just the variables selected above.\n",
    "  saver = tf.train.Saver(vggish_vars, name='vggish_load_pretrained',\n",
    "                         write_version=1)\n",
    "  saver.restore(session, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerry vggish_input.py: start wavfile_to_examples()\n",
      "44100\n",
      "[[789 789]\n",
      " [507 507]\n",
      " [154 154]\n",
      " ...\n",
      " [  0   0]\n",
      " [  0   0]\n",
      " [  0   0]]\n",
      "(432488, 2)\n",
      "Jerry vggish_input.py: start waveform_to_examples()\n",
      "Jerry vggish_input.py: before resample\n",
      "after resample\n",
      "(156912,)\n",
      "Jerry vggish_input.py: before log_mel\n",
      "Jerry vggish_input.py: after log_mel\n",
      "Jerry vggish_input.py: finish waveform_to_examples()\n",
      "(10, 96, 64)\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "import soundfile\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "\n",
    "ckpt = 'vggish_model.ckpt'\n",
    "examples_batch = vggish_input.wavfile_to_examples('test.wav')\n",
    "pproc = vggish_postprocess.Postprocessor()\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # Define the model in inference mode, load the checkpoint, and\n",
    "    # locate input and output tensors.\n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, ckpt)\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    # Run inference and postprocessing.\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "    converter = tf.lite.TFLiteConverter.from_session(sess, [features_tensor], [embedding_tensor])\n",
    "    tflite_model = converter.convert()\n",
    "    open(\"saved_model/vggish_feature_extraction_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 128)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[151,  26, 146, ..., 102, 197, 255],\n",
       "       [154,  26, 168, ...,   0, 138, 255],\n",
       "       [155,  25, 164, ...,  79, 131, 255],\n",
       "       ...,\n",
       "       [175,   8, 146, ..., 147,  69, 255],\n",
       "       [175,   8, 146, ..., 147,  69, 255],\n",
       "       [175,   8, 146, ..., 147,  69, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The original, \"should be\", output after post-process\n",
    "postprocessed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = examples_batch[0]\n",
    "str_input = \"{{\"\n",
    "for lst in sample_input:\n",
    "    str_input += \"{\" + str(round(lst[0],4)) + \"f\"\n",
    "    for num in lst[1:]:\n",
    "        str_input += \",\" + str(round(num, 4))+ \"f\"\n",
    "    str_input += \"},\"\n",
    "str_input = str_input[:-1]\n",
    "str_input += \"}}\"\n",
    "print(str_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.51773083 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.6152872  0.         0.10851269 0.\n",
      "  0.         0.11004636 0.         0.         0.06447921 0.\n",
      "  0.         0.         0.02334112 0.36665872 0.15915178 0.\n",
      "  0.         0.         0.         0.38003775 0.11620829 0.31883034\n",
      "  0.         0.         0.         1.068456   0.35640162 0.\n",
      "  0.         0.         0.8330307  0.         0.         0.\n",
      "  0.         0.90416443 0.         0.19184536 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.3652949  0.\n",
      "  0.         1.1452391  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24345502\n",
      "  0.32170972 0.         0.         0.05670428 0.         0.\n",
      "  0.         0.6894784  0.         0.13961355 0.2678038  0.\n",
      "  0.         0.24759701 0.         1.3265727  0.         0.94142413\n",
      "  0.         0.         0.         0.04135835 0.         0.44179863\n",
      "  0.12995909 0.04707202 0.         0.         0.         0.\n",
      "  0.22660175 0.         0.         0.         0.20387134 0.05225535\n",
      "  0.         0.         0.33169717 0.5493568  0.         0.\n",
      "  0.         0.3283705  0.         0.         0.         0.\n",
      "  0.35539    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Test .tflite model\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"saved_model/vggish_feature_extraction_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array([examples_batch[0]], dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[151,  26, 146,  46, 253,  83, 140,  92, 141, 210, 144,  80, 140,\n",
       "        234, 173, 102,  48, 181, 129, 147,  29, 211,  90, 111, 124, 164,\n",
       "        220, 187,  41,  23, 126, 152, 141, 103,  52, 141, 160,  57,  76,\n",
       "        184, 115,  97,   0, 200,   0, 101,  65, 152, 112, 255, 255,  43,\n",
       "         93, 147, 144, 149, 113,   0, 110, 195, 222,  48,  22, 255, 166,\n",
       "        146, 119, 156, 113,   0, 241, 101, 255, 128, 138, 128,  99, 203,\n",
       "        158, 232,  11, 246, 184, 140, 175, 128, 255, 198,  94,  92,   0,\n",
       "        255, 203, 187,  79, 200,   0,   0, 255, 147, 255,   0, 225,  59,\n",
       "          0, 255, 182, 135,   0, 236, 230, 239,   0,  83, 145,   0, 126,\n",
       "         81,   0, 150,  62,   8,   0, 146, 197, 102, 197, 255]],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pproc.postprocess(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper test for turning python into java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vggish_pca_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_pca_params = \"\"\n",
    "pca_params = vggish_pca_params.pca_eigen_vectors\n",
    "for row in range(len(pca_params)):\n",
    "    if row == 0: java_pca_params += \"{\"\n",
    "    for col in range(len(pca_params[0])):\n",
    "        if col == 0: java_pca_params += \"{\" + str(round(pca_params[row][col],3)) + \"f\"\n",
    "        else: java_pca_params += \",\" + str(round(pca_params[row][col],3)) + \"f\"\n",
    "    java_pca_params += \"},\"\n",
    "java_pca_params = java_pca_params[:-1]\n",
    "java_pca_params += \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"pca_params.txt\", \"w\")\n",
    "pca_params = vggish_pca_params.pca_eigen_vectors\n",
    "for row in range(len(pca_params)):\n",
    "    tmp = \"\"\n",
    "    for col in range(len(pca_params[0])):\n",
    "        tmp += str(round(pca_params[row][col],4)) + \" \"\n",
    "    tmp = tmp[:-1]\n",
    "    f.write(tmp + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_pca_means = \"\"\n",
    "pca_means = vggish_pca_params.pca_means\n",
    "for row in range(len(pca_means)):\n",
    "    if row == 0: java_pca_means += \"{\" + str(round(pca_means[row],3)) + \"f\"\n",
    "    else: java_pca_means += \",\" + str(round(pca_means[row],3)) + \"f\"\n",
    "java_pca_means += \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vggish_pca_params.pca_means).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import vggish_input\n",
    "wav_file = \"testAndroid.wav\"\n",
    "wav_data, sr = vggish_input.wav_read(wav_file)\n",
    "samples = wav_data / 32768.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144384,)\n",
      "(900, 400) (1280, 8) (8,)\n",
      "(900, 64)\n",
      "Jerry vggish_input.py: after log_mel\n",
      "(900, 64)\n",
      "(9, 96, 64) (49152, 512, 8) (512, 8)\n",
      "[-0.64911719 -0.99373337 -1.11742381 -1.86308318 -0.59003545 -0.65416385\n",
      " -0.92818919 -0.84284912 -0.67529392 -0.68415773 -0.65729432 -0.23873038\n",
      " -1.46176886 -1.97142439 -1.63991142 -0.9172873  -1.38036916 -0.97007554\n",
      " -0.2118656  -0.09409409 -0.79908456 -0.17281086 -0.09118928 -0.1033546\n",
      " -0.44792192 -0.2422394  -0.31599703 -0.89422089 -0.85059551 -0.75505449\n",
      " -0.8879782  -1.7242641  -1.88431059 -1.4866908  -2.05265099 -1.80107311\n",
      " -1.78053936 -1.22180689 -0.7424679  -0.28415715 -0.13771165 -1.04611155\n",
      " -0.89409162 -0.69065936 -0.95292212 -1.23092642 -1.31773928 -0.82307479\n",
      " -0.82023966 -0.12395253  0.08214572 -1.22342371 -2.23763686 -3.17392484\n",
      " -3.42293656 -3.35260759 -3.3549805  -3.3102569  -3.18922033 -3.35324471\n",
      " -3.20125546 -2.97750483 -3.22362024 -3.49459519]\n",
      "WARNING:tensorflow:From /home/ohjerry/.local/lib/python3.7/site-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/ohjerry/.local/lib/python3.7/site-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "import soundfile\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim\n",
    "\n",
    "ckpt = 'vggish_model.ckpt'\n",
    "examples_batch = wavfile_to_examples(wav_file)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # Define the model in inference mode, load the checkpoint, and\n",
    "    # locate input and output tensors.\n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, ckpt)\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    # Run inference and postprocessing.\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(embeddings_batch):\n",
    "  \"\"\"Applies postprocessing to a batch of embeddings.\n",
    "\n",
    "  Args:\n",
    "    embeddings_batch: An nparray of shape [batch_size, embedding_size]\n",
    "      containing output from the embedding layer of VGGish.\n",
    "\n",
    "  Returns:\n",
    "    An nparray of the same shape as the input but of type uint8,\n",
    "    containing the PCA-transformed and quantized version of the input.\n",
    "  \"\"\"\n",
    "  pca_matrix = np.array(vggish_pca_params.pca_eigen_vectors)\n",
    "  pca_means = np.array(vggish_pca_params.pca_means).reshape(-1, 1)\n",
    "\n",
    "  pca_applied = np.dot(pca_matrix,\n",
    "                       (embeddings_batch.T - pca_means)).T\n",
    "\n",
    "  # Quantize by:\n",
    "  # - clipping to [min, max] range\n",
    "  clipped_embeddings = np.clip(\n",
    "      pca_applied, vggish_params.QUANTIZE_MIN_VAL,\n",
    "      vggish_params.QUANTIZE_MAX_VAL)\n",
    "\n",
    "  # - convert to 8-bit in range [0.0, 255.0]\n",
    "  quantized_embeddings = (\n",
    "      (clipped_embeddings - vggish_params.QUANTIZE_MIN_VAL) *\n",
    "      (255.0 /\n",
    "       (vggish_params.QUANTIZE_MAX_VAL - vggish_params.QUANTIZE_MIN_VAL)))\n",
    "\n",
    "  # - cast 8-bit float to uint8\n",
    "  quantized_embeddings = quantized_embeddings.astype(np.uint8)\n",
    "  quantized_embeddings = np.asarray(quantized_embeddings, order='C')\n",
    "\n",
    "\n",
    "  print(quantized_embeddings.shape)\n",
    "  print(quantized_embeddings[0])\n",
    "  return quantized_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 128)\n",
      "[154  17 141  77 235  73  94 100 125 215 177  60 140 152 158  47  54  89\n",
      " 157 174  53 223  84 155 119 125 255 163  98 111 149 141 108 165  86  98\n",
      " 136  91  52  80 134 242 112 249 150 133 119  96  88 118 135  51 115  53\n",
      "  97  32 158  45  75 179 188   0  40 255 116 132 122  46 191  97 141 151\n",
      " 185 176  80 115 164 210 115 130  61 157 255  23   3 149 153 146 114  20\n",
      "  60 138  79 255  33 224  69 111 130 168 255  40 230 167  96 255 170 184\n",
      "  48 245 238  14  70  78 191   0 176 171  21  50 146   0   0 188 255 113\n",
      "  99 255]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[154,  17, 141, ..., 113,  99, 255],\n",
       "       [156,  16, 140, ...,  33, 198, 255],\n",
       "       [155,  19, 144, ...,  96, 255, 255],\n",
       "       ...,\n",
       "       [156,  14, 197, ...,  78,  48, 255],\n",
       "       [155,  17, 191, ...,  33,   0, 255],\n",
       "       [160,  19, 193, ...,  29,   0, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess(embedding_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.08134058, 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_batch.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12693973, -0.09922631, -0.1271704 ,  0.02063768, -0.1271704 ,\n",
       "        0.01326802,  0.35762994,  0.46452944,  0.3701476 ])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding_batch.T - np.array(pca_means).reshape(-1,1))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
