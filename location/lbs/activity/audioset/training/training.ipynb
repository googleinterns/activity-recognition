{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd ../dataprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import audio_processing_test as apt\n",
    "import audio_processing as ap\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.dirname(os.path.realpath('__file__')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'example_src_dir'\n",
    "dest_dir = 'example_dest_dir'\n",
    "# filenames should adhere to the following order\n",
    "# [dataset, validation set, test set]\n",
    "filenames = ['test_set']\n",
    "labels = ['Gunshot, gunfire']\n",
    "features_to_extract = ['mfcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dataframes():\n",
    "    length = len(filenames)\n",
    "    if length == 3:\n",
    "        dataset_df = ap.output_df(src_dir, dest_dir, filenames[0], labels, features_to_extract)\n",
    "        evaluation_df = ap.output_df(src_dir, dest_dir, filenames[1], labels, features_to_extract)\n",
    "        validation_df = ap.output_df(src_dir, dest_dir, filenames[2], labels, features_to_extract)\n",
    "        dfs = [dataset_df, evaluation_df, validation_df]\n",
    "    elif length == 2:\n",
    "        dataset_df = ap.output_df(src_dir, dest_dir, filenames[0], labels, features_to_extract)\n",
    "        evaluation_df = ap.output_df(src_dir, dest_dir, filenames[1], labels, features_to_extract)\n",
    "        dfs = [dataset_df, evaluation_df]\n",
    "    elif length == 1:\n",
    "        dataset_df = ap.output_df(src_dir, dest_dir, filenames[0], labels, features_to_extract)\n",
    "        dfs = [dataset_df]\n",
    "    else:\n",
    "        raise ValueError('You must have at least one dataset csv and testing data csv')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = get_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_df = dfs[0]\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert features and classification labels into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset_df.mfcc.tolist(), dtype=object)\n",
    "y = np.array(dataset_df.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_model(dfs, ratio):\n",
    "    length = len(dfs)\n",
    "    if length == 3:\n",
    "        train_x = np.array(dfs[0].mfcc.tolist(), dtype=object)\n",
    "        train_y = np.array(dfs[0].label.tolist())\n",
    "        test_x = np.array(dfs[1].mfcc.tolist(), dtype=object)\n",
    "        test_y = np.array(dfs[1].label.tolist())\n",
    "        val_x = np.array(dfs[2].mfcc.tolist(), dtype=object)\n",
    "        val_y = np.array(dfs[2].label.tolist())\n",
    "        return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "    elif length == 2:\n",
    "        train_x = np.array(dfs[0].mfcc.tolist(), dtype=object)\n",
    "        train_y = np.array(dfs[0].label.tolist())\n",
    "        test_x = np.array(dfs[1].mfcc.tolist(), dtype=object)\n",
    "        test_y = np.array(dfs[1].label.tolist())\n",
    "        return train_x, train_y, test_x, test_y\n",
    "    elif length == 1:\n",
    "        X = np.array(dataset_df.mfcc.tolist(), dtype=object)\n",
    "        y = np.array(dataset_df.label.tolist())\n",
    "        train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.20, random_state = 42)\n",
    "        return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to fix bug:\n",
    "# ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
    "# It worked!!!\n",
    "from keras import backend as K\n",
    "x_train = K.cast_to_floatx(x_train)\n",
    "y_train = K.cast_to_floatx(y_train)\n",
    "x_test = K.cast_to_floatx(x_test)\n",
    "y_test = K.cast_to_floatx(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(history, filename):\n",
    "    history_dict = history.history\n",
    "    history_dict.keys()\n",
    "    acc = history_dict['accuracy']\n",
    "#     val_acc = history_dict['val_accuracy']\n",
    "    loss = history_dict['loss']\n",
    "#     val_loss = history_dict['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7))\n",
    "    \n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    ax1.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "#     ax1.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    ax1.set_title('Training and validation loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(epochs, acc, 'bo', label='Training acc')\n",
    "#     ax2.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    ax2.set_title('Training and validation accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend(loc='lower right')\n",
    "    \n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config1(activation, optimizer, metrics):\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(20,)),\n",
    "        keras.layers.Dense(20, activation=activation),\n",
    "        keras.layers.Dense(1, activation=activation)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train1(model, data, epochs):\n",
    "    x_train, x_test, y_train, y_test = data\n",
    "    history = model.fit(x_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        use_multiprocessing=False\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(history):\n",
    "    loss = history.history.get('loss')\n",
    "    accuracy = history.history.get('accuracy')\n",
    "    tp = history.history.get('tp')\n",
    "    fp = history.history.get('fp')\n",
    "    tn = history.history.get('tn')\n",
    "    fn = history.history.get('fn')\n",
    "    print('Loss: {}'.format(loss))\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    print('True Positives: {}'.format(tp))\n",
    "    print('False Positives: {}'.format(fp))\n",
    "    print('True Negatives: {}'.format(tn))\n",
    "    print('False Negatives: {}'.format(fn))\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    for tp1, fp1 in list(zip(tp, fp)):\n",
    "        prec.append(precision(tp1, fp1))\n",
    "    for tp1, fn1 in list(zip(tp, fn)):\n",
    "        rec.append(recall(tp1, fn1))\n",
    "    for rec1, prec1 in list(zip(rec, prec)):\n",
    "        f1.append(f1score(rec1, prec1))\n",
    "        print('Recall: {}'.format(rec))\n",
    "    print('Precision: {}'.format(prec))\n",
    "    print('F1-Score: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    sum = tp + fp\n",
    "    if sum == 0:\n",
    "        return 0\n",
    "    return tp / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fn):\n",
    "    sum = tp + fn\n",
    "    if sum == 0:\n",
    "        return 0\n",
    "    return tp / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1score(recall, precision):\n",
    "    sum = recall + precision\n",
    "    if sum == 0:\n",
    "        return 0\n",
    "    return 2 * recall * precision / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = os.path.join(dest_dir, 'results1')\n",
    "metrics_list = [['accuracy'], \n",
    "                [tf.keras.metrics.TruePositives(name='tp')], \n",
    "                [tf.keras.metrics.TrueNegatives()], \n",
    "                [tf.keras.metrics.FalseNegatives()], \n",
    "                [tf.keras.metrics.FalsePositives()]\n",
    "               ]\n",
    "metrics = [\n",
    "    'accuracy',\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'),\n",
    "]\n",
    "model = model_config1('softmax', 'adam', metrics)\n",
    "history = model_train1(model, data, 20)\n",
    "visualize_training(history, path)\n",
    "print_performance(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
