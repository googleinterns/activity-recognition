{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigate to the Correct Directory\n",
    "\n",
    "The following code navigates to the dataprocessing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carverforbes/internships/google/gunshot-detection/activity-recognition/location/lbs/activity/audioset/dataprocessing\n"
     ]
    }
   ],
   "source": [
    "cd ../dataprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Import Statements\n",
    "\n",
    "The following code imports the necessary code to run the code in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carverforbes/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran the import statements.\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import audio_processing as ap\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "# tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print(\"Ran the import statements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Configure the following parameters to extract the desired features from a specified csv file to a specific destination directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging to print logging.INFO logs\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments for audio_processing\n",
    "src_dir = 'example_src_dir'\n",
    "dest_dir = 'example_dest_dir'\n",
    "filename = 'Enter_File_Name_Here'\n",
    "labels = ['Gunshot, gunfire']\n",
    "available_features = ['chroma_stft',\n",
    "                       'chroma_cqt',\n",
    "                       'chroma_cens',\n",
    "                       'melspectrogram',\n",
    "                       'mfcc',\n",
    "                       'rms',\n",
    "                       'spectral_centroid',\n",
    "                       'spectral_bandwidth',\n",
    "                       'spectral_contrast',\n",
    "                       'spectral_flatness',\n",
    "                       'spectral_rolloff',\n",
    "                       'poly_features',\n",
    "                       'tonnetz',\n",
    "                       'zero_crossing_rate']\n",
    "features_to_extract = ['desired_feature_to_extract']\n",
    "redo = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_csv(dataframe, dest_path):\n",
    "    df = dataframe.copy()\n",
    "    for column in df.columns:   \n",
    "        for i in range(df[column].size):\n",
    "            if isinstance(df[column][i], np.ndarray):\n",
    "                df[column][i] = df[column][i].tolist()\n",
    "    df.to_csv(path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset as a pandas Dataframe object.\n",
    "csv_path = os.path.join(dest_dir, filename + '.csv')\n",
    "if os.path.isfile(csv_path) and not redo:\n",
    "    df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    df = ap.output_df(src_dir, dest_dir, filename, labels, features_to_extract, redo)\n",
    "    dataframe_to_csv(df, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the first 5 rows of the dataframe.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# copy the dataframe object to perform preprocessing without modifying the original extracted features.\n",
    "train_df_rms = df.copy()\n",
    "train_df_rms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset/dataframe.\n",
    "train_df_rms = train_df_rms.reindex(\n",
    "    np.random.permutation(train_df_rms.index))\n",
    "train_df_rms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp bug fix for rows with None features\n",
    "# also deletes the outer array in the features\n",
    "rows_with_none = []\n",
    "print(train_df_rms.size / 2)\n",
    "# print(train_df_rms.rms[0])\n",
    "for i in train_df_rms.index:\n",
    "    if train_df_rms.rms[i] is None:\n",
    "        rows_with_none.append(i)\n",
    "        continue\n",
    "    train_df_rms.rms[i] = train_df_rms.rms[i][0]\n",
    "print(rows_with_none)\n",
    "train_df_rms = train_df_rms.drop(rows_with_none)\n",
    "train_df_rms.size / 2\n",
    "train_df_rms.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your data preprocessing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_df_rms.rms.tolist(), dtype=object)\n",
    "y = np.array(train_df_rms.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp bug fix for having different sized features\n",
    "temp_x = []\n",
    "temp_y = []\n",
    "count = 0\n",
    "for arr, label in zip(X, y):\n",
    "    if arr is None or len(arr) < 431:\n",
    "#         print('hey look at me', len(arr))\n",
    "        count += 1\n",
    "        continue\n",
    "    temp_x.append(arr)\n",
    "    temp_y.append(label)\n",
    "X = np.array(temp_x, dtype=object)\n",
    "y = np.array(temp_y)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays of objects to arrays of floats.\n",
    "from keras import backend as K\n",
    "x_train = K.cast_to_floatx(x_train)\n",
    "y_train = K.cast_to_floatx(y_train)\n",
    "x_val = K.cast_to_floatx(x_val)\n",
    "y_val = K.cast_to_floatx(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "# Define the plotting function.\n",
    "def plot_curve(epochs, hist, list_of_metrics, path, filename, list_of_hyperparameters):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch and save it to path.\"\"\"  \n",
    "    # list_of_metrics should be one of the names shown in:\n",
    "    # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError as error:\n",
    "            logging.error(error)\n",
    "            \n",
    "    path = os.path.join(path, filename)\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError as error:\n",
    "            logging.error(error)\n",
    "        \n",
    "    list_of_hyperparameters_temp = [str(item) for item in list_of_hyperparameters]\n",
    "    list_of_metrics_temp = [item if isinstance(item, str) else str(item.name) for item in list_of_metrics]\n",
    "    filename = '_'.join(list_of_metrics_temp) + '_' + '_'.join(list_of_hyperparameters_temp)\n",
    "    path = os.path.join(path, filename + '.png')\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    \n",
    "    return plt\n",
    "\n",
    "\n",
    "print(\"Defined the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 1\n",
    "Neural Network Type:\n",
    "* Input Layer: (Number of Nodes)\n",
    "* Hidden Layer 1:\n",
    "* Hidden Layer 2: \n",
    "* Ouput Layer: \n",
    "\n",
    "Hyper-parameters:\n",
    "* Loss Function: \n",
    "* Activation Function: \n",
    "* Optimizer Function: \n",
    "* Learning Rate: \n",
    "* Epochs: \n",
    "* Batch_Size: \n",
    "* Classification Threshold: \n",
    "* Regularization: \n",
    "* Regularization Lambda: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, optimizer, regularization):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    # Most simple tf.keras models are sequential.\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of ___ nodes\n",
    "    model.add(tf.keras.layers.Dense(units=, input_shape=(,)))\n",
    "              \n",
    "    # Implement ___ regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "    \n",
    "    # Implement ___ regularization in the second hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.  Notice that we're using a different loss\n",
    "    # function for classification than for regression.    \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # The x parameter of tf.keras.Model.fit can be a list of arrays.\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "\n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "\n",
    "    # Isolate the classification metric for each epoch.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# Here is the updated definition of METRICS:\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(\n",
    "          name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(\n",
    "          thresholds=classification_threshold, name='precision'),\n",
    "      tf.keras.metrics.Recall(\n",
    "          thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, METRICS)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(my_model, x_train, y_train, epochs, \n",
    "                          label_name, batch_size, validation_data=(x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "plot_curve(epochs, hist, ['loss', 'val_loss'])\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(\n",
    "          name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(\n",
    "          thresholds=classification_threshold, name='precision'),\n",
    "      tf.keras.metrics.Recall(\n",
    "          thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
