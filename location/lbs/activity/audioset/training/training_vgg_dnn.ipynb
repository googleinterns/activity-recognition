{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigate to the Correct Directory\n",
    "\n",
    "The following code navigates to the dataprocessing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/carverforbes/activity-recognition/location/lbs/activity/audioset/dataprocessing\n"
     ]
    }
   ],
   "source": [
    "cd ../dataprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Import Statements\n",
    "\n",
    "The following code imports the necessary code to run the code in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran the import statements.\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import audio_processing as ap\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "# tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print(\"Ran the import statements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory should be arranged in form:\n",
    "# .\n",
    "# ├── audioset_v1_embeddings\n",
    "# ├── class_labels_indices.csv\n",
    "# └── Model_on_VGG.ipynb\n",
    "\n",
    "src_dir = \"example_src_dir\"\n",
    "dest_dir = 'example_dest_dir'\n",
    "path = \"audioset_v1_embeddings\"\n",
    "eva = \"eval\"\n",
    "bal = \"bal_train\"\n",
    "unbal = \"unbal_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Configure the following parameters to extract the desired features from a specified csv file to a specific destination directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging to print logging.INFO logs\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(filename):\n",
    "    # Read in a tfrecord file\n",
    "    # Store information in list of lists\n",
    "    # Audio_embedding is a list of 10 embeddings, each represents 1 second feature\n",
    "    #\n",
    "    # input: str filename\n",
    "    # output: pandas dataframe with columns:\n",
    "    #        [str video_id, float start_time, float end_time, list label_index, list embed]\n",
    "    \n",
    "    if not filename.endswith('.tfrecord'):\n",
    "        print(\"This file is not a .tfrecord file.\")\n",
    "        return\n",
    "    raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "    return_df = pd.DataFrame(columns=\n",
    "                             ['video_id', 'start_time_seconds', 'end_time_seconds', 'labels', 'audio_embedding'])\n",
    "    for raw_record in raw_dataset:\n",
    "        cur_record_list = []\n",
    "        example = tf.train.SequenceExample()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        \n",
    "        cur_record_list.append(example.context.feature['video_id'].bytes_list.value[0].decode(\"utf-8\"))\n",
    "        cur_record_list.append(example.context.feature['start_time_seconds'].float_list.value[0])\n",
    "        cur_record_list.append(example.context.feature['end_time_seconds'].float_list.value[0])\n",
    "        cur_record_list.append(example.context.feature['labels'].int64_list.value)\n",
    "        \n",
    "        # Original embeddings are stored in hex format, now convert them to readable int\n",
    "        embeds = []\n",
    "        for i in range(len(example.feature_lists.feature_list['audio_embedding'].feature)):\n",
    "            hexembed = example.feature_lists.feature_list['audio_embedding'].feature[i].bytes_list.value[0].hex()\n",
    "            arrayembed = [int(hexembed[i:i+2], 16) for i in range(0, len(hexembed), 2)]\n",
    "            embeds.append(arrayembed)\n",
    "        cur_record_list.append(embeds)\n",
    "        \n",
    "        return_df.loc[len(return_df)] = cur_record_list\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_tfrecords(dir_path):\n",
    "    final_df = pd.DataFrame(\n",
    "        columns=['video_id', 'start_time_seconds', 'end_time_seconds', 'labels', 'audio_embedding'])\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".tfrecord\"):\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            df = read_tfrecord(file_path)\n",
    "            final_df = final_df.append(df, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_positive_examples(dataframe, positive_labels, csv_path):\n",
    "    # output set of label ids to use as positive labels\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "    positive_labels = set(positive_labels)\n",
    "    positive_indices = set()\n",
    "    for index in labels_df.index:\n",
    "        if labels_df['display_name'][index] in positive_labels:\n",
    "            positive_indices.add(index)\n",
    "    for index in dataframe.index:\n",
    "        positive = False\n",
    "        for label in dataframe.labels[index]:\n",
    "            if label in positive_indices:\n",
    "                positive = True\n",
    "                dataframe.labels[index] = 1\n",
    "            if not positive:\n",
    "                dataframe.labels[index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_positive(dataframe):\n",
    "    count = 0\n",
    "    for index in dataframe.index:\n",
    "        if dataframe.labels[index] == 1:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(dataframe, ratio):\n",
    "    positive_df = pd.DataFrame(columns=['label', 'feature'])\n",
    "    negative_df = pd.DataFrame(columns=['label', 'feature'])\n",
    "    for index in dataframe.index:\n",
    "        if dataframe.label[index] == 1:\n",
    "            temp = pd.DataFrame([[1, dataframe.feature[index]]], columns=['label', 'feature'])\n",
    "            positive_df = positive_df.append(temp, ignore_index=True)\n",
    "        elif dataframe.label[index] == 0:\n",
    "            temp = pd.DataFrame([[0, dataframe.feature[index]]], columns=['label', 'feature'])\n",
    "            negative_df = negative_df.append(temp, ignore_index=True)\n",
    "    positive_df = positive_df.append(negative_df.sample(len(positive_df) * ratio), ignore_index=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_seconds(dataframe):\n",
    "    df = pd.DataFrame(columns=['label', 'feature'])\n",
    "    for index in dataframe.index:\n",
    "        for arr in dataframe['audio_embedding'][index]:\n",
    "            temp = pd.DataFrame([[dataframe.labels[index], arr]], columns=['label', 'feature'])\n",
    "            df = df.append(temp, ignore_index=True)                \n",
    "        print(df.index.stop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(dataframe):\n",
    "    for index in dataframe.index:\n",
    "        arr = np.array(dataframe['feature'][index]) / 255\n",
    "        dataframe['feature'][index] = arr\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataframe(dataframe):\n",
    "    # Shuffle the dataset/dataframe.\n",
    "    dataframe = dataframe.reindex(np.random.permutation(dataframe.index))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_x_y(dataframe):\n",
    "    X = np.array(dataframe.feature.tolist(), dtype=object)\n",
    "    y = np.array(dataframe.label.tolist())\n",
    "    \n",
    "    # Convert arrays of objects to arrays of floats.\n",
    "    X = tf.keras.backend.cast_to_floatx(X)\n",
    "    y = tf.keras.backend.cast_to_floatx(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_from_dataset(dataset_path, class_labels_csv_path, label_list, ratio=1):\n",
    "    print('Getting dataframe from .tfrecord files')\n",
    "    df = get_vgg_tfrecords(dataset_path)\n",
    "    print('Setting positive examples to label 1')\n",
    "    label_positive_examples(df, label_list, class_labels_csv_path)\n",
    "    print('balancing dataset')\n",
    "    df = balance_dataset(df, ratio)\n",
    "    print('shuffling dataset')\n",
    "    df = shuffle_dataframe(df)\n",
    "    print('normalizing features')\n",
    "    df = normalize_features(df)\n",
    "    print('splitting data into 1 second intervals')\n",
    "    df = separate_seconds(df)\n",
    "    X, y = dataframe_to_x_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_from_dataset_list(dataset_path_list, class_labels_csv_path, label_list, ratio=1):\n",
    "    length = 0 if dataset_path_list is None else len(dataset_path_list)\n",
    "    if length <= 0 or length > 3:\n",
    "        print('Cannot have less than one training set and more than a training, validation, and test set')\n",
    "        return None\n",
    "    elif length == 1:\n",
    "        dataset_path = dataset_path_list[0]\n",
    "        X, y = get_x_y_from_dataset(dataset_path, class_labels_csv_path, label_list, ratio)\n",
    "        x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "        return x_train, y_train, x_val, y_val\n",
    "    elif length == 2:\n",
    "        train_path = dataset_path_list[0]\n",
    "        val_path = dataset_path_list[1]\n",
    "        x_train, y_train = get_x_y_from_dataset(train_path, class_labels_csv_path, label_list, ratio)\n",
    "        x_val, y_val = get_x_y_from_dataset(val_path, class_labels_csv_path, label_list, ratio)\n",
    "        return x_train, y_train, x_val, y_val\n",
    "    else:\n",
    "        train_path = dataset_path_list[0]\n",
    "        val_path = dataset_path_list[1]\n",
    "        test_path = dataset_path_list[2]\n",
    "        x_train, y_train = get_x_y_from_dataset(train_path, class_labels_csv_path, label_list, ratio)\n",
    "        x_val, y_val = get_x_y_from_dataset(val_path, class_labels_csv_path, label_list, ratio)\n",
    "        x_test, y_test = get_x_y_from_dataset(test_path, class_labels_csv_path, label_list, ratio)\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "# Define the plotting function.\n",
    "def plot_curve(epochs, hist, list_of_metrics, path, filename, list_of_hyperparameters):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch and save it to path.\"\"\"  \n",
    "    # list_of_metrics should be one of the names shown in:\n",
    "    # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError as error:\n",
    "            logging.error(error)\n",
    "            \n",
    "    path = os.path.join(path, filename)\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError as error:\n",
    "            logging.error(error)\n",
    "        \n",
    "    list_of_hyperparameters_temp = [str(item) for item in list_of_hyperparameters]\n",
    "    list_of_metrics_temp = [item if isinstance(item, str) else str(item.name) for item in list_of_metrics]\n",
    "    filename = '_'.join(list_of_metrics_temp) + '_' + '_'.join(list_of_hyperparameters_temp)\n",
    "    path = os.path.join(path, filename + '.png')\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    \n",
    "    return plt\n",
    "\n",
    "\n",
    "print(\"Defined the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 1\n",
    "Neural Network Type:\n",
    "* Input Layer: (Number of Nodes)\n",
    "* Hidden Layer 1:\n",
    "* Hidden Layer 2: \n",
    "* Ouput Layer: \n",
    "\n",
    "Hyper-parameters:\n",
    "* Loss Function: \n",
    "* Activation Function: \n",
    "* Optimizer Function: \n",
    "* Learning Rate: \n",
    "* Epochs: \n",
    "* Batch_Size: \n",
    "* Classification Threshold: \n",
    "* Regularization: \n",
    "* Regularization Lambda: \n",
    "\n",
    "Notes:\n",
    "* Data was balanced with a 1:20 ratio between positive and negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, activation, optimizer, regularization, regularization_lambda):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    # Most simple tf.keras models are sequential.\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of ___ nodes\n",
    "    model.add(tf.keras.layers.Dense(units=128, input_shape=(128,)))\n",
    "              \n",
    "    # Implement ___ regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=128, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "    \n",
    "#     # Implement ___ regularization in the second hidden layer.\n",
    "#     model.add(tf.keras.layers.Dense(units=128, \n",
    "#                                   activation=activation,\n",
    "#                                   kernel_regularizer=regularization(regularization_lambda),\n",
    "#                                   name='Hidden2'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.  Notice that we're using a different loss\n",
    "    # function for classification than for regression.    \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # The x parameter of tf.keras.Model.fit can be a list of arrays.\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "\n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "\n",
    "    # Isolate the classification metric for each epoch.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "ratio_list = [1, 20, 40]\n",
    "activation_list = ['elu', 'exponential', 'relu', 'selu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh']\n",
    "optimizer_list = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "filename = 'balanced_train_segments'\n",
    "dest_dir = 'example_dest_dir'\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# Here is the updated definition of METRICS:\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(\n",
    "          name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(\n",
    "          thresholds=classification_threshold, name='precision'),\n",
    "      tf.keras.metrics.Recall(\n",
    "          thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, METRICS, activation, optimizer, regularization, regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(\n",
    "    my_model, x_train, y_train, epochs, label_name, batch_size, validation_data=(\n",
    "        x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot, dest_dir, filename, list_of_hyperparameters)\n",
    "plot_curve(epochs, hist, ['loss', 'val_loss'], dest_dir, filename, list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 2\n",
    "Neural Network Type:\n",
    "* Input Layer: (Number of Nodes)\n",
    "* Hidden Layer 1:\n",
    "* Hidden Layer 2: \n",
    "* Ouput Layer: \n",
    "\n",
    "Hyper-parameters:\n",
    "* Loss Function: \n",
    "* Activation Function: \n",
    "* Optimizer Function: \n",
    "* Learning Rate: \n",
    "* Epochs: \n",
    "* Batch_Size: \n",
    "* Classification Threshold: \n",
    "* Regularization: \n",
    "* Regularization Lambda: \n",
    "\n",
    "Notes:\n",
    "* Data was balanced with a 1:20 ratio between positive and negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, activation, optimizer, regularization, regularization_lambda):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    # Most simple tf.keras models are sequential.\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of ___ nodes\n",
    "    model.add(tf.keras.layers.Dense(units=128, input_shape=(128,)))\n",
    "              \n",
    "    # Implement ___ regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=128, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "    \n",
    "    # Include a dropout layer.\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    # Implement ___ regularization in the second hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=128, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.  Notice that we're using a different loss\n",
    "    # function for classification than for regression.    \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # The x parameter of tf.keras.Model.fit can be a list of arrays.\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "\n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "\n",
    "    # Isolate the classification metric for each epoch.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "ratio_list = [1, 20, 40]\n",
    "activation_list = ['elu', 'exponential', 'relu', 'selu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh']\n",
    "optimizer_list = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "filename = 'balanced_train_segments'\n",
    "dest_dir = 'example_dest_dir'\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# Here is the updated definition of METRICS:\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(\n",
    "          name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(\n",
    "          thresholds=classification_threshold, name='precision'),\n",
    "      tf.keras.metrics.Recall(\n",
    "          thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, METRICS, activation, optimizer, regularization, regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(\n",
    "    my_model, x_train, y_train, epochs, label_name, batch_size, validation_data=(\n",
    "        x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot, dest_dir, filename, list_of_hyperparameters)\n",
    "plot_curve(epochs, hist, ['loss', 'val_loss'], dest_dir, filename, list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 3\n",
    "Neural Network Type:\n",
    "* Input Layer: (Number of Nodes)\n",
    "* Hidden Layer 1:\n",
    "* Hidden Layer 2: \n",
    "* Ouput Layer: \n",
    "\n",
    "Hyper-parameters:\n",
    "* Loss Function: \n",
    "* Activation Function: \n",
    "* Optimizer Function: \n",
    "* Learning Rate: \n",
    "* Epochs: \n",
    "* Batch_Size: \n",
    "* Classification Threshold: \n",
    "* Regularization: \n",
    "* Regularization Lambda: \n",
    "\n",
    "Notes:\n",
    "* Data was balanced with a 1: ratio between positive and negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataframe from .tfrecord files\n"
     ]
    }
   ],
   "source": [
    "bal_path = os.path.join(src_dir, path, bal)\n",
    "unbal_path = os.path.join(src_dir, path, unbal)\n",
    "eva_path = os.path.join(src_dir, path, eva)\n",
    "class_labels_path = os.path.join(src_dir, 'class_labels_indices.csv')\n",
    "labels_to_be_positive = ['Gunshot, gunfire', 'Machine gun', 'Fusillade', 'Artillery fire']\n",
    "dataset_path_list = [unbal_path, bal_path, eva_path]\n",
    "ratio = 1\n",
    "\n",
    "data = get_x_y_from_dataset_list(\n",
    "    dataset_path_list, class_labels_path, labels_to_be_positive, ratio)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, activation, optimizer, regularization, regularization_lambda):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    # Most simple tf.keras models are sequential.\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of ___ nodes\n",
    "    model.add(tf.keras.layers.Dense(units=128, input_shape=(128,)))\n",
    "              \n",
    "    # Implement ___ regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=128, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "    \n",
    "#     # Implement ___ regularization in the second hidden layer.\n",
    "#     model.add(tf.keras.layers.Dense(units=128, \n",
    "#                                   activation=activation,\n",
    "#                                   kernel_regularizer=regularization(regularization_lambda),\n",
    "#                                   name='Hidden2'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.  Notice that we're using a different loss\n",
    "    # function for classification than for regression.    \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # The x parameter of tf.keras.Model.fit can be a list of arrays.\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "\n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "\n",
    "    # Isolate the classification metric for each epoch.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "ratio_list = [1, 20, 40]\n",
    "activation_list = ['elu', 'exponential', 'relu', 'selu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh']\n",
    "optimizer_list = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "filename = 'balanced_train_segments'\n",
    "dest_dir = 'example_dest_dir'\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# Here is the updated definition of METRICS:\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(\n",
    "          name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(\n",
    "          thresholds=classification_threshold, name='precision'),\n",
    "      tf.keras.metrics.Recall(\n",
    "          thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(\n",
    "    learning_rate, METRICS, activation, optimizer, regularization, regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(\n",
    "    my_model, x_train, y_train, epochs, label_name, batch_size, validation_data=(\n",
    "        x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", \n",
    "                           'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(\n",
    "    epochs, hist, list_of_metrics_to_plot, dest_dir, filename, list_of_hyperparameters)\n",
    "plot_curve(\n",
    "    epochs, hist, ['loss', 'val_loss'], dest_dir, filename, list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
