{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigate to the Correct Directory\n",
    "\n",
    "The following code navigates to the dataprocessing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cd ../dataprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Import Statements\n",
    "\n",
    "The following code imports the necessary code to run the code in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import audio_processing as ap\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "print(\"Ran the import statements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory should be arranged in form:\n",
    "# .\n",
    "# ├── audioset_v1_embeddings\n",
    "# ├── class_labels_indices.csv\n",
    "# └── Model_on_VGG.ipynb\n",
    "\n",
    "src_dir = \"example_src_dir\"\n",
    "dest_dir = 'example_dest_dir'\n",
    "path = \"audioset_v1_embeddings\"\n",
    "eva = \"eval\"\n",
    "bal = \"bal_train\"\n",
    "unbal = \"unbal_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Configure the following parameters to extract the desired features from a specified csv file to a specific destination directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging to print logging.INFO logs\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(filename):\n",
    "    # Read in a tfrecord file\n",
    "    # Store information in list of lists\n",
    "    # Audio_embedding is a list of 10 embeddings, each represents 1 second feature\n",
    "    #\n",
    "    # input: str filename\n",
    "    # output: pandas dataframe with columns:\n",
    "    #        [str video_id, float start_time, float end_time, list label_index, list embed]\n",
    "    \n",
    "    if not filename.endswith('.tfrecord'):\n",
    "        print(\"This file is not a .tfrecord file.\")\n",
    "        return\n",
    "    raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "    return_df = pd.DataFrame(columns=\n",
    "                             ['video_id', 'start_time_seconds', 'end_time_seconds', 'labels', 'audio_embedding'])\n",
    "    for raw_record in raw_dataset:\n",
    "        cur_record_list = []\n",
    "        example = tf.train.SequenceExample()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        \n",
    "        cur_record_list.append(example.context.feature['video_id'].bytes_list.value[0].decode(\"utf-8\"))\n",
    "        cur_record_list.append(example.context.feature['start_time_seconds'].float_list.value[0])\n",
    "        cur_record_list.append(example.context.feature['end_time_seconds'].float_list.value[0])\n",
    "        cur_record_list.append(example.context.feature['labels'].int64_list.value)\n",
    "        \n",
    "        # Original embeddings are stored in hex format, now convert them to readable int\n",
    "        embeds = []\n",
    "        for i in range(len(example.feature_lists.feature_list['audio_embedding'].feature)):\n",
    "            hexembed = example.feature_lists.feature_list['audio_embedding'].feature[i].bytes_list.value[0].hex()\n",
    "            arrayembed = [int(hexembed[i:i+2], 16) for i in range(0, len(hexembed), 2)]\n",
    "            embeds.append(arrayembed)\n",
    "        cur_record_list.append(embeds)\n",
    "        \n",
    "        return_df.loc[len(return_df)] = cur_record_list\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_tfrecords(dir_path):\n",
    "    final_df = pd.DataFrame(\n",
    "        columns=['video_id', 'start_time_seconds', 'end_time_seconds', 'labels', 'audio_embedding'])\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".tfrecord\"):\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            df = read_tfrecord(file_path)\n",
    "            final_df = final_df.append(df, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_positive_examples(dataframe, positive_labels, csv_path):\n",
    "    # output set of label ids to use as positive labels\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "    positive_labels = set(positive_labels)\n",
    "    positive_indices = set()\n",
    "    for index in labels_df.index:\n",
    "        if labels_df['display_name'][index] in positive_labels:\n",
    "            positive_indices.add(index)\n",
    "    for index in dataframe.index:\n",
    "        positive = False\n",
    "        for label in dataframe.labels[index]:\n",
    "            if label in positive_indices:\n",
    "                positive = True\n",
    "                dataframe.labels[index] = 1\n",
    "                break\n",
    "        if not positive:\n",
    "            dataframe.labels[index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_positive(dataframe):\n",
    "    count = 0\n",
    "    for index in dataframe.index:\n",
    "        if dataframe.labels[index] == 1:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(dataframe, ratio):\n",
    "    positive_df = pd.DataFrame(columns=['label', 'feature'])\n",
    "    negative_df = pd.DataFrame(columns=['label', 'feature'])\n",
    "    for index in dataframe.index:\n",
    "        if dataframe.labels[index] == 1:\n",
    "            temp = pd.DataFrame([[1, dataframe['audio_embedding'][index]]], columns=['label', 'feature'])\n",
    "            positive_df = positive_df.append(temp, ignore_index=True)\n",
    "        elif dataframe.labels[index] == 0:\n",
    "            temp = pd.DataFrame([[0, dataframe['audio_embedding'][index]]], columns=['label', 'feature'])\n",
    "            negative_df = negative_df.append(temp, ignore_index=True)\n",
    "    positive_df = positive_df.append(negative_df.sample(len(positive_df) * ratio), ignore_index=True)\n",
    "    return positive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_seconds(dataframe):\n",
    "    df = pd.DataFrame(columns=['label', 'feature'])\n",
    "    for index in dataframe.index:\n",
    "        for arr in dataframe.feature[index]:\n",
    "            if len(arr) == 128:  \n",
    "                temp = pd.DataFrame([[dataframe.label[index], arr]], columns=['label', 'feature'])\n",
    "                df = df.append(temp, ignore_index=True)                \n",
    "        print(df.index.stop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(dataframe):\n",
    "    for index in dataframe.index:\n",
    "        arr = np.array(dataframe['feature'][index]) / 255\n",
    "        dataframe['feature'][index] = arr\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataframe(dataframe):\n",
    "    # Shuffle the dataset/dataframe.\n",
    "    dataframe = dataframe.reindex(np.random.permutation(dataframe.index))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_x_y(dataframe):\n",
    "    X = np.array(dataframe.feature.tolist(), dtype=object)\n",
    "    y = np.array(dataframe.label.tolist())\n",
    "    \n",
    "    # Convert arrays of objects to arrays of floats.\n",
    "    X = tf.keras.backend.cast_to_floatx(X)\n",
    "    y = tf.keras.backend.cast_to_floatx(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_from_dataset(dataset_path, class_labels_csv_path, label_list, ratio=1):\n",
    "    print('Getting dataframe from .tfrecord files')\n",
    "    df = get_vgg_tfrecords(dataset_path)\n",
    "    print('Setting positive examples to label 1')\n",
    "    label_positive_examples(df, label_list, class_labels_csv_path)\n",
    "    print('balancing dataset')\n",
    "    df = balance_dataset(df, ratio)\n",
    "    print('shuffling dataset')\n",
    "    df = shuffle_dataframe(df)\n",
    "    print('normalizing features')\n",
    "    df = normalize_features(df)\n",
    "    print('splitting data into 1 second intervals')\n",
    "    df = separate_seconds(df)\n",
    "    X, y = dataframe_to_x_y(df)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_from_dataset_list(dataset_path_list, class_labels_csv_path, label_list, ratio=1):\n",
    "    length = 0 if dataset_path_list is None else len(dataset_path_list)\n",
    "    if length <= 0 or length > 3:\n",
    "        print('Cannot have less than one training set and more than a training, validation, and test set')\n",
    "        return None\n",
    "    elif length == 1:\n",
    "        dataset_path = dataset_path_list[0]\n",
    "        X, y = get_x_y_from_dataset(dataset_path, class_labels_csv_path, label_list, ratio)\n",
    "        x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "        return x_train, y_train, x_val, y_val\n",
    "    elif length == 2:\n",
    "        train_path = dataset_path_list[0]\n",
    "        val_path = dataset_path_list[1]\n",
    "        x_train, y_train = get_x_y_from_dataset(train_path, class_labels_csv_path, label_list, ratio)\n",
    "        x_val, y_val = get_x_y_from_dataset(val_path, class_labels_csv_path, label_list, ratio)\n",
    "        return x_train, y_train, x_val, y_val\n",
    "    else:\n",
    "        train_path = dataset_path_list[0]\n",
    "        val_path = dataset_path_list[1]\n",
    "        test_path = dataset_path_list[2]\n",
    "        x_train, y_train = get_x_y_from_dataset(train_path, class_labels_csv_path, label_list, ratio)\n",
    "        x_val, y_val = get_x_y_from_dataset(val_path, class_labels_csv_path, label_list, ratio)\n",
    "        x_test, y_test = get_x_y_from_dataset(test_path, class_labels_csv_path, label_list, ratio)\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bal_path = os.path.join(src_dir, path, bal)\n",
    "unbal_path = os.path.join(src_dir, path, unbal)\n",
    "eva_path = os.path.join(src_dir, path, eva)\n",
    "class_labels_path = os.path.join(src_dir, 'class_labels_indices.csv')\n",
    "labels_to_be_positive = ['Gunshot, gunfire', 'Machine gun', 'Fusillade', 'Artillery fire']\n",
    "dataset_path_list = [unbal_path, bal_path, eva_path]\n",
    "ratio = 1\n",
    "\n",
    "data = get_x_y_from_dataset_list(\n",
    "    dataset_path_list, class_labels_path, labels_to_be_positive, ratio)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function.\n",
    "def plot_curve(epochs, hist, list_of_metrics, path, filename, list_of_hyperparameters):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch and save it to path.\"\"\"  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError as error:\n",
    "            logging.error(error)\n",
    "            \n",
    "    path = os.path.join(path, filename)\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError as error:\n",
    "            logging.error(error)\n",
    "        \n",
    "    list_of_hyperparameters_temp = [str(item) for item in list_of_hyperparameters]\n",
    "    list_of_metrics_temp = [item if isinstance(item, str) else str(item.name) for item in list_of_metrics]\n",
    "    filename = '_'.join(list_of_metrics_temp) + '_' + '_'.join(list_of_hyperparameters_temp)\n",
    "    path = os.path.join(path, filename + '.png')\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    \n",
    "    return plt\n",
    "\n",
    "\n",
    "print(\"Defined the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, activation, optimizer, regularization, regularization_lambda):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of 128 nodes\n",
    "    model.add(tf.keras.layers.Dense(units=128, input_shape=(128,)))\n",
    "              \n",
    "    # Implement L2 regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=128, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.    \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "filename = 'balanced_train_segments'\n",
    "dest_dir = 'example_dest_dir'\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# Metrics to measure model performance\n",
    "METRICS = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "    tf.keras.metrics.Precision(thresholds=classification_threshold, name='precision'),\n",
    "    tf.keras.metrics.Recall(thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Create model\n",
    "my_model = create_model(learning_rate, METRICS, activation, optimizer, regularization, regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(\n",
    "    my_model, x_train, y_train, epochs, label_name, batch_size, validation_data=(\n",
    "        x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot, dest_dir, filename, list_of_hyperparameters)\n",
    "plot_curve(epochs, hist, ['loss', 'val_loss'], dest_dir, filename, list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, activation, optimizer, regularization, regularization_lambda):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of 128 nodes\n",
    "    model.add(tf.keras.layers.Dense(units=128, input_shape=(128,)))\n",
    "              \n",
    "    # Implement L2 regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=128, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "    \n",
    "    # Include a dropout layer.\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    # Implement L2 regularization in the second hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=128, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.   \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "filename = 'balanced_train_segments'\n",
    "dest_dir = 'example_dest_dir'\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# METRICS to measure the performance of the model:\n",
    "METRICS = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "    tf.keras.metrics.Precision(thresholds=classification_threshold, name='precision'),\n",
    "    tf.keras.metrics.Recall(thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Create model\n",
    "my_model = create_model(learning_rate, METRICS, activation, optimizer, regularization, regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(\n",
    "    my_model, x_train, y_train, epochs, label_name, batch_size, validation_data=(\n",
    "        x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, list_of_metrics_to_plot, list_of_hyperparameters)\n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, ['loss', 'val_loss'], list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
