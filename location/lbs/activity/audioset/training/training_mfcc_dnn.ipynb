{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigate to the Correct Directory\n",
    "\n",
    "The following code navigates to the dataprocessing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/carverforbes/activity-recognition/location/lbs/activity/audioset/dataprocessing\n"
     ]
    }
   ],
   "source": [
    "cd ../dataprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Import Statements\n",
    "\n",
    "The following code imports the necessary code to run the code in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran the import statements.\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import audio_processing as ap\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "from absl import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "print(\"Ran the import statements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Configure the following parameters to extract the desired features from a specified csv file to a specific destination directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging to print logging.INFO logs\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments for audio_processing\n",
    "src_dir = 'example_src_dir'\n",
    "dest_dir = 'example_dest_dir'\n",
    "filename = 'gunshot_50_50'\n",
    "notebook = 'training_mfcc_dnn'\n",
    "labels = ['Gunshot, gunfire']\n",
    "available_features = ['chroma_stft',\n",
    "                       'chroma_cqt',\n",
    "                       'chroma_cens',\n",
    "                       'melspectrogram',\n",
    "                       'mfcc',\n",
    "                       'rms',\n",
    "                       'spectral_centroid',\n",
    "                       'spectral_bandwidth',\n",
    "                       'spectral_contrast',\n",
    "                       'spectral_flatness',\n",
    "                       'spectral_rolloff',\n",
    "                       'poly_features',\n",
    "                       'tonnetz',\n",
    "                       'zero_crossing_rate']\n",
    "features_to_extract = ['mfcc']\n",
    "redo = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_csv(dataframe, dest_path):\n",
    "    start_time = datetime.datetime.now()\n",
    "    stop = dataframe.index.stop\n",
    "    begin = 0\n",
    "    end = 100\n",
    "    count = 0\n",
    "    while end < stop + 100:\n",
    "        df = dataframe.iloc[begin:end, :].copy()\n",
    "        for column in df.columns:\n",
    "            for i in range(df[column].size):\n",
    "                i = i + count * 100\n",
    "                if isinstance(df[column][i], np.ndarray):\n",
    "                    df[column][i] = df[column][i].tolist()\n",
    "        if count == 0:\n",
    "            df.to_csv(dest_path, index=False, header=True)\n",
    "        else:\n",
    "            df.to_csv(dest_path, mode='a', index=False, header=True)\n",
    "        begin += 100\n",
    "        end += 100\n",
    "        count += 1\n",
    "    end_time = datetime.datetime.now()\n",
    "    function_duration = end_time - start_time\n",
    "    \n",
    "        \n",
    "    print('Created the csv file at the destination path in {} seconds.'.format(\n",
    "        function_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dataframe(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for column in df.columns:\n",
    "        for i in range(df[column].size):\n",
    "            if isinstance(df[column][i], str):\n",
    "                temp = df[column][i][1:-1]\n",
    "                temp = temp.replace(',', ' ').split(']')\n",
    "                new_list = []\n",
    "                for item in temp:\n",
    "                    if item == '':\n",
    "                        continue\n",
    "                    item = item.replace('[', '').strip().split()\n",
    "                    item = [float(num) for num in item]\n",
    "                    arr = np.array(item)\n",
    "                    new_list.append(arr)\n",
    "                new_arr = np.array(new_list)\n",
    "                df[column][i] = new_arr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the dataset as a pandas DataFrame object.\n",
    "df = ap.output_df(src_dir, dest_dir, filename, labels, features_to_extract, redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>mfcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[[-303.27164, -310.85724, -324.51694, -325.635...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[-282.00607, -289.7648, -299.44608, -298.7115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[[-601.3228, -601.3228, -601.3228, -601.3228, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[[-244.3129, -188.5689, -145.10178, -155.65407...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[[-305.8128, -300.2478, -302.55612, -301.78333...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               mfcc\n",
       "0      1  [[-303.27164, -310.85724, -324.51694, -325.635...\n",
       "1      1  [[-282.00607, -289.7648, -299.44608, -298.7115...\n",
       "2      1  [[-601.3228, -601.3228, -601.3228, -601.3228, ...\n",
       "3      1  [[-244.3129, -188.5689, -145.10178, -155.65407...\n",
       "4      0  [[-305.8128, -300.2478, -302.55612, -301.78333..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows of the dataframe.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your data preprocessing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dataframe(dataframe):\n",
    "    # temp bug fix for rows with None features\n",
    "    # and rows with different number of elements per frame\n",
    "    bad_rows = []\n",
    "    for i in dataframe.index:\n",
    "        if dataframe.mfcc[i] is None:\n",
    "            bad_rows.append(i)\n",
    "        elif dataframe.mfcc[i][0] is None:\n",
    "            bad_rows.append(i)\n",
    "        elif len(dataframe.mfcc[i]) != 20:\n",
    "            bad_rows.append(i)\n",
    "        elif len(dataframe.mfcc[i][0]) != 431:\n",
    "            bad_rows.append(i)\n",
    "    new_dataframe = dataframe.drop(bad_rows)\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataframe(dataframe):\n",
    "    # Shuffle the dataset/dataframe.\n",
    "    dataframe = dataframe.reindex(np.random.permutation(dataframe.index))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_features(dataframe):\n",
    "    # Flatten melspectrogram's (128, 431) shaped features\n",
    "    for i in dataframe.index:\n",
    "        dataframe.mfcc[i] = dataframe.mfcc[i].flatten()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    temp_x = []\n",
    "    for arr in features:\n",
    "        norm = np.linalg.norm(arr)\n",
    "        arr = arr / norm\n",
    "        temp_x.append(arr)\n",
    "    features = np.array(temp_x, dtype=object)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_x_y(dataframe):\n",
    "    X = np.array(dataframe.mfcc.tolist(), dtype=object)\n",
    "    y = np.array(dataframe.label.tolist())\n",
    "    \n",
    "    # Convert arrays of objects to arrays of floats.\n",
    "    X = tf.keras.backend.cast_to_floatx(X)\n",
    "    y = tf.keras.backend.cast_to_floatx(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataframe):\n",
    "    print('Dropping incongruent features from dataframe')\n",
    "    dataframe = fix_dataframe(dataframe)\n",
    "    print('Flatten features in the dataframe')\n",
    "    dataframe = flatten_features(dataframe)\n",
    "    print('shuffling dataframe')\n",
    "    dataframe = shuffle_dataframe(dataframe)\n",
    "    print('Getting X, y out of dataframe')\n",
    "    X, y = dataframe_to_x_y(dataframe)\n",
    "    x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping incongruent features from dataframe\n",
      "Flatten features in the dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-f128fc5641ce>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.mfcc[i] = dataframe.mfcc[i].flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling dataframe\n",
      "Getting X, y out of dataframe\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val = data_preprocessing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError as error:\n",
    "            logging.error(error)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "# Define the plotting function.\n",
    "def plot_curve(epochs, hist, dest_path, notebook_filename, dataset_filename, list_of_metrics, list_of_hyperparameters):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch and save it to path.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    make_dir(dest_path)    \n",
    "    path = os.path.join(dest_path, notebook_filename)\n",
    "    make_dir(path)\n",
    "    path = os.path.join(path, dataset_filename)\n",
    "    make_dir(path)\n",
    "        \n",
    "    list_of_hyperparameters_temp = [str(item) for item in list_of_hyperparameters]\n",
    "    filename = '_'.join(list_of_hyperparameters_temp)\n",
    "    path = os.path.join(path, filename + '.png')\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    \n",
    "    return plt\n",
    "\n",
    "\n",
    "print(\"Defined the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the create_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, optimizer, regularization, regularization_lambda):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of 8620 nodes\n",
    "    model.add(tf.keras.layers.Dense(units=8620, input_shape=(8620,)))\n",
    "              \n",
    "    # Implement L2 regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=8620, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.    \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8620)              74313020  \n",
      "_________________________________________________________________\n",
      "Hidden1 (Dense)              (None, 8620)              74313020  \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 8621      \n",
      "=================================================================\n",
      "Total params: 148,634,661\n",
      "Trainable params: 148,634,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "351/351 [==============================] - 265s 756ms/step - loss: 2171.5479 - accuracy: 0.5059 - precision: 0.4655 - recall: 0.3735 - val_loss: 34.8981 - val_accuracy: 0.5527 - val_precision: 0.5130 - val_recall: 0.0600\n",
      "Epoch 2/100\n",
      "351/351 [==============================] - 264s 752ms/step - loss: 550.2348 - accuracy: 0.5130 - precision: 0.4789 - recall: 0.4559 - val_loss: 15.2691 - val_accuracy: 0.5513 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/100\n",
      "351/351 [==============================] - 263s 748ms/step - loss: 679.5317 - accuracy: 0.5070 - precision: 0.4739 - recall: 0.4824 - val_loss: 6.9181 - val_accuracy: 0.5545 - val_precision: 0.5036 - val_recall: 0.4995\n",
      "Epoch 4/100\n",
      "351/351 [==============================] - 257s 731ms/step - loss: 452.9714 - accuracy: 0.5018 - precision: 0.4684 - recall: 0.4746 - val_loss: 65.1152 - val_accuracy: 0.4998 - val_precision: 0.4221 - val_recall: 0.3113\n",
      "Epoch 5/100\n",
      "351/351 [==============================] - 242s 689ms/step - loss: 357.0847 - accuracy: 0.5137 - precision: 0.4811 - recall: 0.4922 - val_loss: 39.4804 - val_accuracy: 0.5513 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/100\n",
      "351/351 [==============================] - 261s 742ms/step - loss: 361.3363 - accuracy: 0.5107 - precision: 0.4775 - recall: 0.4783 - val_loss: 2940.9961 - val_accuracy: 0.4496 - val_precision: 0.4490 - val_recall: 0.9990\n",
      "Epoch 7/100\n",
      "351/351 [==============================] - 416s 1s/step - loss: 504.9388 - accuracy: 0.5122 - precision: 0.4787 - recall: 0.4707 - val_loss: 82.9176 - val_accuracy: 0.5513 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/100\n",
      "351/351 [==============================] - 408s 1s/step - loss: 440.2557 - accuracy: 0.5136 - precision: 0.4779 - recall: 0.4222 - val_loss: 20.8060 - val_accuracy: 0.5513 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/100\n",
      "351/351 [==============================] - 447s 1s/step - loss: 377.9955 - accuracy: 0.5306 - precision: 0.4986 - recall: 0.4795 - val_loss: 3.4711 - val_accuracy: 0.5500 - val_precision: 0.2857 - val_recall: 0.0020\n",
      "Epoch 10/100\n",
      "351/351 [==============================] - 566s 2s/step - loss: 261.2584 - accuracy: 0.5173 - precision: 0.4837 - recall: 0.4593 - val_loss: 1489.9764 - val_accuracy: 0.5513 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/100\n",
      "351/351 [==============================] - 534s 2s/step - loss: 490.7230 - accuracy: 0.5285 - precision: 0.4964 - recall: 0.4905 - val_loss: 49.3723 - val_accuracy: 0.5495 - val_precision: 0.3571 - val_recall: 0.0051\n",
      "Epoch 12/100\n",
      "351/351 [==============================] - 577s 2s/step - loss: 434.3522 - accuracy: 0.5240 - precision: 0.4916 - recall: 0.4910 - val_loss: 207.5709 - val_accuracy: 0.4943 - val_precision: 0.4697 - val_recall: 0.9847\n",
      "Epoch 13/100\n",
      "351/351 [==============================] - 455s 1s/step - loss: 395.5298 - accuracy: 0.5241 - precision: 0.4912 - recall: 0.4625 - val_loss: 159.9620 - val_accuracy: 0.5509 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/100\n",
      "351/351 [==============================] - 485s 1s/step - loss: 322.4095 - accuracy: 0.5138 - precision: 0.4797 - recall: 0.4559 - val_loss: 175.0324 - val_accuracy: 0.5513 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/100\n",
      "351/351 [==============================] - 516s 1s/step - loss: 336.3003 - accuracy: 0.5370 - precision: 0.5056 - recall: 0.4956 - val_loss: 258.0988 - val_accuracy: 0.5436 - val_precision: 0.0526 - val_recall: 0.0010\n",
      "Epoch 16/100\n",
      "351/351 [==============================] - 538s 2s/step - loss: 410.5856 - accuracy: 0.5427 - precision: 0.5120 - recall: 0.4941 - val_loss: 70.3894 - val_accuracy: 0.4975 - val_precision: 0.4710 - val_recall: 0.9756\n",
      "Epoch 17/100\n",
      "351/351 [==============================] - 456s 1s/step - loss: 364.5905 - accuracy: 0.5453 - precision: 0.5151 - recall: 0.4917 - val_loss: 83.5086 - val_accuracy: 0.6157 - val_precision: 0.5486 - val_recall: 0.8098\n",
      "Epoch 18/100\n",
      "351/351 [==============================] - 485s 1s/step - loss: 347.8557 - accuracy: 0.5394 - precision: 0.5093 - recall: 0.4391 - val_loss: 23.3574 - val_accuracy: 0.5144 - val_precision: 0.4789 - val_recall: 0.9329\n",
      "Epoch 19/100\n",
      "351/351 [==============================] - 467s 1s/step - loss: 326.1711 - accuracy: 0.5471 - precision: 0.5175 - recall: 0.4817 - val_loss: 249.2745 - val_accuracy: 0.5513 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 20/100\n",
      "351/351 [==============================] - 417s 1s/step - loss: 345.3668 - accuracy: 0.5478 - precision: 0.5190 - recall: 0.4661 - val_loss: 15.7888 - val_accuracy: 0.5103 - val_precision: 0.4773 - val_recall: 0.9644\n",
      "Epoch 21/100\n",
      "351/351 [==============================] - 292s 832ms/step - loss: 351.5972 - accuracy: 0.5412 - precision: 0.5112 - recall: 0.4566 - val_loss: 75.4255 - val_accuracy: 0.5623 - val_precision: 0.5066 - val_recall: 0.9430\n",
      "Epoch 22/100\n",
      "351/351 [==============================] - 271s 772ms/step - loss: 435.1313 - accuracy: 0.5388 - precision: 0.5091 - recall: 0.4149 - val_loss: 699.0298 - val_accuracy: 0.5500 - val_precision: 0.4400 - val_recall: 0.0112\n",
      "Epoch 23/100\n",
      "351/351 [==============================] - 347s 989ms/step - loss: 522.3494 - accuracy: 0.5415 - precision: 0.5124 - recall: 0.4288 - val_loss: 133.5435 - val_accuracy: 0.5929 - val_precision: 0.5274 - val_recall: 0.8922\n",
      "Epoch 24/100\n",
      "351/351 [==============================] - 299s 853ms/step - loss: 340.4424 - accuracy: 0.5393 - precision: 0.5092 - recall: 0.4386 - val_loss: 83.6722 - val_accuracy: 0.5472 - val_precision: 0.2632 - val_recall: 0.0051\n",
      "Epoch 25/100\n",
      "274/351 [======================>.......] - ETA: 59s - loss: 564.0337 - accuracy: 0.5537 - precision: 0.5254 - recall: 0.4988 "
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# A list of metrics to measure the performance of the model:\n",
    "METRICS = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "    tf.keras.metrics.Precision(thresholds=classification_threshold, name='precision'),\n",
    "    tf.keras.metrics.Recall(thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Create model\n",
    "my_model = create_model(learning_rate, METRICS, optimizer=optimizer, regularization=regularization, regularization_lambda=regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(my_model, x_train, y_train, epochs, \n",
    "                          label_name, batch_size, validation_data=(x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, list_of_metrics_to_plot, list_of_hyperparameters)\n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, ['loss', 'val_loss'], list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# A list of metrics to measure the performance of the model:\n",
    "METRICS = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "    tf.keras.metrics.Precision(thresholds=classification_threshold, name='precision'),\n",
    "    tf.keras.metrics.Recall(thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Create model\n",
    "my_model = create_model(learning_rate, METRICS, optimizer=optimizer, regularization=regularization, regularization_lambda=regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(my_model, x_train, y_train, epochs, \n",
    "                          label_name, batch_size, validation_data=(x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, list_of_metrics_to_plot, list_of_hyperparameters)\n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, ['loss', 'val_loss'], list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, optimizer, regularization, regularization_lambda):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of 8620 nodes\n",
    "    model.add(tf.keras.layers.Dense(units=8620, input_shape=(8620,)))\n",
    "              \n",
    "    # Implement L2 regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=8620, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "    # Include a dropout layer.\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    # Implement L2 regularization in the second hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=8620, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.   \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# A list of metrics to measure the performance of the model:\n",
    "METRICS = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "    tf.keras.metrics.Precision(thresholds=classification_threshold, name='precision'),\n",
    "    tf.keras.metrics.Recall(thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Create model\n",
    "my_model = create_model(learning_rate, METRICS, optimizer=optimizer, regularization=regularization, regularization_lambda=regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(my_model, x_train, y_train, epochs, \n",
    "                          label_name, batch_size, validation_data=(x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, list_of_metrics_to_plot, list_of_hyperparameters)\n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, ['loss', 'val_loss'], list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, my_metrics, optimizer, regularization, regularization_lambda):\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    # Discard any pre-existing version of the model.\n",
    "    model = None\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the input layer of 8620 nodes\n",
    "    model.add(tf.keras.layers.Dense(units=8620, input_shape=(8620,)))\n",
    "              \n",
    "    # Implement L2 regularization in the first hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=8620, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden1'))\n",
    "    # Include a dropout layer.\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    # Implement L2 regularization in the second hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=8620, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden2'))\n",
    "    \n",
    "    # Include a dropout layer.\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    # Implement L2 regularization in the second hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=1000, \n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=regularization(regularization_lambda),\n",
    "                                  name='Hidden3'))\n",
    "\n",
    "    # Funnel the regression value through a sigmoid function.\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid,\n",
    "                                  name='Output'))\n",
    "\n",
    "    # Call the compile method to construct the layers into a model that\n",
    "    # TensorFlow can execute.   \n",
    "    model.compile(optimizer=optimizer(lr=my_learning_rate),                                                   \n",
    "                loss=loss,\n",
    "                metrics=my_metrics)\n",
    "\n",
    "    return model        \n",
    "              \n",
    "def train_model(model, features, label, epochs, label_name,\n",
    "                batch_size=None, my_validation_split=0.0,\n",
    "                validation_data=None, shuffle=True):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, validation_data=validation_data)\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "activation = 'relu'\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "classification_threshold = 0.70\n",
    "regularization = tf.keras.regularizers.l2\n",
    "regularization_lambda = 0.001\n",
    "label_name = \"label\"\n",
    "\n",
    "list_of_hyperparameters = [learning_rate, epochs, batch_size,\n",
    "                           classification_threshold,\n",
    "                           regularization_lambda,\n",
    "                           label_name]\n",
    "\n",
    "\n",
    "# A list of metrics to measure the performance of the model:\n",
    "METRICS = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "    tf.keras.metrics.Precision(thresholds=classification_threshold, name='precision'),\n",
    "    tf.keras.metrics.Recall(thresholds=classification_threshold, name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Create model\n",
    "my_model = create_model(learning_rate, METRICS, optimizer=optimizer, regularization=regularization, regularization_lambda=regularization_lambda)\n",
    "\n",
    "# View the model's structure.\n",
    "my_model.summary()\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(my_model, x_train, y_train, epochs, \n",
    "                          label_name, batch_size, validation_data=(x_val, y_val))\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\", 'val_accuracy', 'val_precision', 'val_recall'] \n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, list_of_metrics_to_plot, list_of_hyperparameters)\n",
    "plot_curve(epochs, hist, dest_dir, notebook, filename, ['loss', 'val_loss'], list_of_hyperparameters)\n",
    "\n",
    "training_performance =  my_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Training Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', training_performance[0])\n",
    "print('accuracy: ', training_performance[1])\n",
    "print('precision: ', training_performance[2])\n",
    "print('recall: ', training_performance[3])\n",
    "print()\n",
    "\n",
    "validation_performance =  my_model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Validation Performance')\n",
    "print('---------------------------------')\n",
    "print('loss: ', validation_performance[0])\n",
    "print('accuracy: ', validation_performance[1])\n",
    "print('precision: ', validation_performance[2])\n",
    "print('recall: ', validation_performance[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
